# Marco 2 - Fase 7: Test Applications - Documenta√ß√£o de Implementa√ß√£o

**Data:** 2026-01-28
**Status:** ‚úÖ COMPLETO (com ressalva TLS)
**Executor:** DevOps Team + Claude Sonnet 4.5
**Framework:** [executor-terraform.md](../../../../../docs/prompts/executor-terraform.md)

---

## üìã Sum√°rio Executivo

Fase 7 implementa valida√ß√£o end-to-end da plataforma Kubernetes atrav√©s do deploy de aplica√ß√µes de teste (NGINX e Echo Server) expostas via AWS Application Load Balancer. A implementa√ß√£o valida a integra√ß√£o completa do stack: Ingress Controller ‚Üí ALB ‚Üí Network Policies ‚Üí Pods ‚Üí Prometheus Metrics ‚Üí Loki Logs.

**Status Atual:**
- ‚úÖ **4 pods Running** (2 NGINX com sidecar exporter, 2 Echo Server)
- ‚úÖ **2 ALBs ativos** respondendo HTTP 200
- ‚úÖ **Prometheus integration** funcionando (ServiceMonitors auto-discovery)
- ‚úÖ **Loki logs** sendo coletados via Fluent Bit
- ‚ö†Ô∏è **TLS temporariamente removido** devido a problema com dom√≠nios fake (.local) sem DNS real

---

## üéØ Objetivos da Fase 7

### Prim√°rios
1. ‚úÖ **Validar AWS Load Balancer Controller** - Provisionamento autom√°tico de ALBs a partir de Ingress resources
2. ‚úÖ **Validar Network Policies** - Tr√°fego permitido: ALB ‚Üí Pods, Prometheus ‚Üí Metrics endpoints
3. ‚úÖ **Validar Prometheus Integration** - ServiceMonitors funcionando, m√©tricas sendo scraped
4. ‚úÖ **Validar Loki Logging** - Fluent Bit coletando logs de todos os pods
5. ‚ö†Ô∏è **Validar TLS/Cert-Manager** - PENDENTE (problema identificado, solu√ß√£o planejada)

### Secund√°rios
6. ‚úÖ **Testar kubectl Terraform provider** - Aplicar manifests Kubernetes via gavinbunney/kubectl
7. ‚úÖ **Validar nodeSelector** - Pods agendados corretamente no node group "workloads"
8. ‚úÖ **Testar Sidecar Pattern** - NGINX + NGINX Exporter funcionando em conjunto

---

## üèóÔ∏è Arquitetura Implementada

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Internet                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ  AWS ALB (HTTP)  ‚îÇ  ‚Üê ALB Controller auto-provision
                ‚îÇ  Port 80         ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ nginx-test     ‚îÇ              ‚îÇ echo-server      ‚îÇ
‚îÇ Service        ‚îÇ              ‚îÇ Service          ‚îÇ
‚îÇ ClusterIP      ‚îÇ              ‚îÇ ClusterIP        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ nginx-test     ‚îÇ              ‚îÇ echo-server      ‚îÇ
‚îÇ Deployment     ‚îÇ              ‚îÇ Deployment       ‚îÇ
‚îÇ 2 replicas     ‚îÇ              ‚îÇ 2 replicas       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                                 ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ         ‚îÇ                      ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê                ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê
‚îÇPod  ‚îÇ  ‚îÇPod  ‚îÇ                ‚îÇPod  ‚îÇ  ‚îÇPod  ‚îÇ
‚îÇnginx‚îÇ  ‚îÇnginx‚îÇ                ‚îÇecho ‚îÇ  ‚îÇecho ‚îÇ
‚îÇ+exp ‚îÇ  ‚îÇ+exp ‚îÇ                ‚îÇ     ‚îÇ  ‚îÇ     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ        ‚îÇ                      ‚îÇ        ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                                ‚îÇ
        ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  Prometheus  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  (ServiceMonitor)
                  ‚îÇ  Scraping    ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                                 ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Grafana  ‚îÇ                    ‚îÇ   Loki   ‚îÇ
   ‚îÇDashboard ‚îÇ                    ‚îÇ  Logs    ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                         ‚îÇ
                                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                   ‚îÇ Fluent Bit‚îÇ
                                   ‚îÇ DaemonSet ‚îÇ
                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ Recursos Implementados

### 1. Terraform Module: test-applications

**Localiza√ß√£o:** `modules/test-applications/`

#### Arquivos:
- **main.tf** (133 linhas)
  - Namespace creation (`test-apps`)
  - kubectl manifest application (via `for_each`)
  - Network Policy (allow ALB + Prometheus)
- **variables.tf** (28 linhas)
  - `cluster_name` (string)
  - `namespace` (string, default: "test-apps")
  - `tags` (map)
- **outputs.tf** (18 linhas)
  - `namespace_name`
  - `nginx_manifests_count`
  - `echo_manifests_count`
- **versions.tf** (27 linhas)
  - `kubectl` provider ~> 1.14

#### C√≥digo-Chave (main.tf):
```hcl
# Namespace com labels para identifica√ß√£o
resource "kubernetes_namespace" "test_apps" {
  metadata {
    name = var.namespace
    labels = {
      name                       = var.namespace
      "app.kubernetes.io/name"   = "test-applications"
      "app.kubernetes.io/part-of" = "marco2-fase7"
    }
  }
}

# Apply NGINX manifests usando kubectl provider
data "kubectl_file_documents" "nginx_test" {
  content = file("${path.module}/manifests/nginx-test.yaml")
}

resource "kubectl_manifest" "nginx_test" {
  for_each  = data.kubectl_file_documents.nginx_test.manifests
  yaml_body = each.value
  depends_on = [kubernetes_namespace.test_apps]
}

# Network Policy: Permitir ingress de kube-system (ALB) + monitoring (Prometheus)
resource "kubernetes_network_policy" "allow_ingress_monitoring" {
  metadata {
    name      = "allow-alb-and-monitoring"
    namespace = kubernetes_namespace.test_apps.metadata[0].name
  }

  spec {
    pod_selector {}
    policy_types = ["Ingress", "Egress"]

    # Ingress from kube-system (ALB Controller)
    ingress {
      from {
        namespace_selector {
          match_labels = { name = "kube-system" }
        }
      }
      ports {
        protocol = "TCP"
        port     = "80"
      }
      ports {
        protocol = "TCP"
        port     = "8080"
      }
      ports {
        protocol = "TCP"
        port     = "9113"  # NGINX Exporter metrics
      }
    }

    # Ingress from monitoring (Prometheus)
    ingress {
      from {
        namespace_selector {
          match_labels = { name = "monitoring" }
        }
      }
      ports {
        protocol = "TCP"
        port     = "9113"  # NGINX Exporter metrics
      }
    }

    # Egress: DNS, API Server, Internet
    egress {
      to {
        namespace_selector {
          match_labels = { name = "kube-system" }
        }
      }
      ports {
        protocol = "UDP"
        port     = "53"  # DNS
      }
    }
  }
}
```

---

### 2. Kubernetes Manifests

#### A. nginx-test.yaml (145 linhas)

**Componentes:**
1. **Deployment**
   - Replicas: 2
   - NodeSelector: `node-type: workloads`
   - Containers:
     - **nginx:** `nginx:1.27-alpine` (port 80)
     - **nginx-exporter:** `nginx/nginx-prometheus-exporter:1.4.0` (port 9113)
   - Resources:
     - NGINX: 100m CPU / 128Mi RAM (requests), 200m / 256Mi (limits)
     - Exporter: 50m CPU / 64Mi RAM (requests), 100m / 128Mi (limits)

2. **Service**
   - Type: ClusterIP
   - Ports:
     - `http`: 80 ‚Üí targetPort 80 (NGINX)
     - `metrics`: 9113 ‚Üí targetPort 9113 (Exporter)
   - Selector: `app: nginx-test`

3. **ServiceMonitor** (Prometheus Operator CRD)
   - Endpoint: port `metrics` (9113)
   - Path: `/metrics`
   - Interval: 30s
   - Selector: `app: nginx-test`

4. **Ingress**
   - IngressClass: `alb`
   - Annotations:
     - `alb.ingress.kubernetes.io/scheme: internet-facing`
     - `alb.ingress.kubernetes.io/target-type: ip`
     - `alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'`
     - `alb.ingress.kubernetes.io/healthcheck-path: /`
   - Rules: Path `/` ‚Üí Service `nginx-test:80`

**C√≥digo Completo:**
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-test
  namespace: test-apps
  labels:
    app: nginx-test
    app.kubernetes.io/name: nginx-test
    app.kubernetes.io/component: web
    app.kubernetes.io/part-of: test-applications
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-test
  template:
    metadata:
      labels:
        app: nginx-test
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9113"
        prometheus.io/path: "/metrics"
    spec:
      nodeSelector:
        node-type: workloads
      containers:
      - name: nginx
        image: nginx:1.27-alpine
        ports:
        - containerPort: 80
          name: http
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/conf.d/default.conf
          subPath: default.conf

      # Sidecar: NGINX Prometheus Exporter
      - name: nginx-exporter
        image: nginx/nginx-prometheus-exporter:1.4.0
        args:
        - -nginx.scrape-uri=http://localhost:8080/stub_status
        ports:
        - containerPort: 9113
          name: metrics
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 100m
            memory: 128Mi

      volumes:
      - name: nginx-config
        configMap:
          name: nginx-test-config

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-test-config
  namespace: test-apps
data:
  default.conf: |
    server {
        listen 80;
        server_name _;

        location / {
            root /usr/share/nginx/html;
            index index.html;
        }

        # Stub status para NGINX Exporter
        location /stub_status {
            stub_status on;
            access_log off;
            allow 127.0.0.1;
            deny all;
        }
    }

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-test
  namespace: test-apps
  labels:
    app: nginx-test
spec:
  type: ClusterIP
  selector:
    app: nginx-test
  ports:
  - name: http
    port: 80
    targetPort: http
    protocol: TCP
  - name: metrics
    port: 9113
    targetPort: metrics
    protocol: TCP

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: nginx-test
  namespace: test-apps
  labels:
    app: nginx-test
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      app: nginx-test
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-test-ingress
  namespace: test-apps
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
    alb.ingress.kubernetes.io/healthcheck-path: /
spec:
  ingressClassName: alb
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-test
            port:
              number: 80
```

---

#### B. echo-server.yaml (115 linhas)

**Componentes:**
1. **Deployment**
   - Replicas: 2
   - NodeSelector: `node-type: workloads`
   - Container: `ealen/echo-server:latest`
   - Environment: `PORT=8080`
   - Resources: 50m CPU / 64Mi RAM (requests), 100m / 128Mi (limits)

2. **Service**
   - Type: ClusterIP
   - Port: 8080

3. **ServiceMonitor**
   - Endpoint: port 8080
   - Path: `/metrics` (se dispon√≠vel, sen√£o ignora)

4. **Ingress**
   - Similar ao nginx-test
   - Healthcheck: `/`

**C√≥digo Completo:**
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: echo-server
  namespace: test-apps
  labels:
    app: echo-server
    app.kubernetes.io/name: echo-server
    app.kubernetes.io/component: api
    app.kubernetes.io/part-of: test-applications
spec:
  replicas: 2
  selector:
    matchLabels:
      app: echo-server
  template:
    metadata:
      labels:
        app: echo-server
    spec:
      nodeSelector:
        node-type: workloads
      containers:
      - name: echo-server
        image: ealen/echo-server:latest
        ports:
        - containerPort: 8080
          name: http
        env:
        - name: PORT
          value: "8080"
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 100m
            memory: 128Mi

---
apiVersion: v1
kind: Service
metadata:
  name: echo-server
  namespace: test-apps
  labels:
    app: echo-server
spec:
  type: ClusterIP
  selector:
    app: echo-server
  ports:
  - name: http
    port: 8080
    targetPort: http
    protocol: TCP

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: echo-server
  namespace: test-apps
  labels:
    app: echo-server
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      app: echo-server
  endpoints:
  - port: http
    path: /
    interval: 30s

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: echo-server-ingress
  namespace: test-apps
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
    alb.ingress.kubernetes.io/healthcheck-path: /
spec:
  ingressClassName: alb
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: echo-server
            port:
              number: 8080
```

---

### 3. Provider Configuration

**Arquivo:** `marco2/providers.tf`

**Adicionado kubectl provider:**
```hcl
terraform {
  required_providers {
    # ... (outros providers)
    kubectl = {
      source  = "gavinbunney/kubectl"
      version = "~> 1.14"
    }
  }
}

provider "kubectl" {
  host                   = data.aws_eks_cluster.cluster.endpoint
  cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority[0].data)
  load_config_file       = false

  exec {
    api_version = "client.authentication.k8s.io/v1beta1"
    command     = "aws"
    args = [
      "eks",
      "get-token",
      "--cluster-name",
      var.cluster_name,
      "--region",
      var.region
    ]
  }
}
```

**Por qu√™ kubectl provider?**
- Suporta CRDs (ServiceMonitor) nativamente
- Melhor handling de manifests multi-document YAML
- Detec√ß√£o autom√°tica de resource types
- Menos verbose que `kubernetes_manifest` (Hashicorp provider)

---

### 4. Integration no Marco 2

**Arquivo:** `marco2/main.tf`

```hcl
module "test_applications" {
  source = "./modules/test-applications"

  cluster_name = var.cluster_name
  namespace    = "test-apps"

  tags = {
    Environment = "test"
    Project     = "k8s-platform"
    Marco       = "marco2"
    Fase        = "7"
    ManagedBy   = "terraform"
  }

  depends_on = [module.cluster_autoscaler]
}
```

**Dependency Chain:**
```
module.kube_prometheus_stack (Fase 3)
  ‚Üì
module.loki + module.fluent_bit (Fase 4)
  ‚Üì
module.network_policies (Fase 5)
  ‚Üì
module.cluster_autoscaler (Fase 6)
  ‚Üì
module.test_applications (Fase 7)  ‚Üê VOC√ä EST√Å AQUI
```

---

## üöÄ Deploy Execution

### Pr√©-requisitos Verificados
```bash
# 1. AWS credentials
aws sts get-caller-identity
# Account: 891377105802

# 2. EKS cluster acess√≠vel
aws eks update-kubeconfig --region us-east-1 --name k8s-platform-prod
kubectl get nodes
# 7 nodes Ready

# 3. Dependencies (Fases 1-6) operacionais
kubectl get pods -n kube-system | grep aws-load-balancer-controller  # Running
kubectl get pods -n cert-manager  # 3 pods Running
kubectl get pods -n monitoring    # 13 pods Running (Prometheus + Loki)
kubectl get networkpolicies -A    # 11 policies
kubectl get pods -n kube-system | grep cluster-autoscaler  # Running
```

---

### Terraform Apply

```bash
cd platform-provisioning/aws/kubernetes/terraform/envs/marco2

# Init
terraform init -upgrade
# Initializing modules...
# - test_applications in ./modules/test-applications
# Initializing provider plugins...
# - gavinbunney/kubectl v1.14.0
# Terraform has been successfully initialized!

# Validate
terraform validate
# Success! The configuration is valid.

# Plan
terraform plan
# Plan: 11 to add, 0 to change, 0 to destroy.
#
# Terraform will perform the following actions:
#   + kubernetes_namespace.test_apps
#   + kubernetes_network_policy.allow_ingress_monitoring
#   + kubectl_manifest.nginx_test["0"] (Deployment)
#   + kubectl_manifest.nginx_test["1"] (ConfigMap)
#   + kubectl_manifest.nginx_test["2"] (Service)
#   + kubectl_manifest.nginx_test["3"] (ServiceMonitor)
#   + kubectl_manifest.nginx_test["4"] (Ingress)
#   + kubectl_manifest.echo_server["0"] (Deployment)
#   + kubectl_manifest.echo_server["1"] (Service)
#   + kubectl_manifest.echo_server["2"] (ServiceMonitor)
#   + kubectl_manifest.echo_server["3"] (Ingress)

# Apply
terraform apply
```

**Output:**
```
module.test_applications.kubernetes_namespace.test_apps: Creating...
module.test_applications.kubernetes_namespace.test_apps: Creation complete after 1s

module.test_applications.kubectl_manifest.nginx_test["0"]: Creating...
module.test_applications.kubectl_manifest.nginx_test["1"]: Creating...
module.test_applications.kubectl_manifest.nginx_test["2"]: Creating...
module.test_applications.kubectl_manifest.nginx_test["3"]: Creating...
module.test_applications.kubectl_manifest.nginx_test["4"]: Creating...
module.test_applications.kubectl_manifest.echo_server["0"]: Creating...
module.test_applications.kubectl_manifest.echo_server["1"]: Creating...
module.test_applications.kubectl_manifest.echo_server["2"]: Creating...
module.test_applications.kubectl_manifest.echo_server["3"]: Creating...

module.test_applications.kubernetes_network_policy.allow_ingress_monitoring: Creating...
module.test_applications.kubernetes_network_policy.allow_ingress_monitoring: Creation complete after 2s

module.test_applications.kubectl_manifest.nginx_test["0"]: Creation complete after 12s
module.test_applications.kubectl_manifest.nginx_test["1"]: Creation complete after 5s
module.test_applications.kubectl_manifest.nginx_test["2"]: Creation complete after 6s
module.test_applications.kubectl_manifest.nginx_test["3"]: Creation complete after 7s
module.test_applications.kubectl_manifest.nginx_test["4"]: Creation complete after 8s

module.test_applications.kubectl_manifest.echo_server["0"]: Creation complete after 13s
module.test_applications.kubectl_manifest.echo_server["1"]: Creation complete after 6s
module.test_applications.kubectl_manifest.echo_server["2"]: Creation complete after 7s
module.test_applications.kubectl_manifest.echo_server["3"]: Creation complete after 8s

Apply complete! Resources: 11 added, 0 changed, 0 destroyed.
```

**Tempo Total:** ~45 segundos

---

### Corre√ß√µes Durante Deploy

#### Issue #1: ImagePullBackOff (Echo Server)

**Erro:**
```bash
kubectl get pods -n test-apps
# NAME                           READY   STATUS             RESTARTS   AGE
# echo-server-6987564-7mqfb      0/1     ImagePullBackOff   0          2m
```

**Diagn√≥stico:**
```bash
kubectl describe pod echo-server-6987564-7mqfb -n test-apps
# Events:
#   Warning  Failed     1m   kubelet  Failed to pull image "ealen/echo-server:0.9.4": not found
```

**Root Cause:** Vers√£o `0.9.4` n√£o existe no Docker Hub.

**Fix:**
```bash
# Atualizar manifest
sed -i 's/ealen\/echo-server:0.9.4/ealen\/echo-server:latest/g' \
  modules/test-applications/manifests/echo-server.yaml

# Apply diretamente (bypass Terraform para hotfix)
kubectl apply -f modules/test-applications/manifests/echo-server.yaml
```

**Resultado:** Pods iniciaram com sucesso (1/1 Running).

---

#### Issue #2: TLS Blocking ALB Creation (CR√çTICO)

**Sintomas:**
```bash
kubectl get ingress -n test-apps
# NAME                   CLASS   HOSTS   ADDRESS   PORTS     AGE
# nginx-test-ingress     alb     *                 80, 443   5m
# echo-server-ingress    alb     *                 80, 443   5m
#
# Esperado: ADDRESS com ALB DNS name
# Atual: ADDRESS vazio (ALB n√£o provisionado)
```

**Diagn√≥stico:**
```bash
# Logs ALB Controller
kubectl logs -n kube-system deployment/aws-load-balancer-controller --tail=50
# error: ingress: test-apps/nginx-test-ingress: no certificate found for host: nginx-test.test-apps.local
# error: failed to build LoadBalancer configuration: no certificate found

# Certificate status
kubectl get certificate -n test-apps
# NAME              READY   SECRET            AGE
# nginx-test-tls    False   nginx-test-tls    5m
# echo-server-tls   False   echo-server-tls   5m

kubectl describe certificate nginx-test-tls -n test-apps
# Status:
#   Conditions:
#     Type:    Ready
#     Status:  False
#     Reason:  DoesNotExist
#     Message: Certificate does not exist
```

**Root Cause Analysis:**
1. Ingress configurado com `tls` section e hosts `.local` (dom√≠nios fake)
2. Annotation `cert-manager.io/cluster-issuer: selfsigned-issuer` presente
3. Cert-Manager tentou criar Certificate resources
4. Self-signed issuer falhou (optimistic locking issues, config problems)
5. ALB Controller exige certificados reais para HTTPS listeners quando TLS section existe
6. AWS ALB API retornou erro: `ValidationError: A certificate must be specified for HTTPS listeners`
7. ALB Controller bloqueou provisionamento do ALB inteiro (n√£o criou nem HTTP listener)

**Tentativas de Fix (Progressivas):**

**Tentativa #1:** Remover annotation cert-manager
```bash
kubectl annotate ingress nginx-test-ingress -n test-apps cert-manager.io/cluster-issuer-
kubectl annotate ingress echo-server-ingress -n test-apps cert-manager.io/cluster-issuer-
```
**Resultado:** ‚ùå Falhou - ALB Controller ainda esperava certificados por causa do `tls` section

**Tentativa #2:** Remover TLS section inteira
```bash
kubectl patch ingress nginx-test-ingress -n test-apps --type=json \
  -p='[{"op": "remove", "path": "/spec/tls"}]'

kubectl patch ingress echo-server-ingress -n test-apps --type=json \
  -p='[{"op": "remove", "path": "/spec/tls"}]'
```
**Resultado:** ‚ö†Ô∏è Parcial - ALB criou LoadBalancer mas ainda tentou HTTPS listener

**Tentativa #3:** Alterar listen-ports para HTTP-only
```bash
kubectl annotate ingress nginx-test-ingress -n test-apps \
  alb.ingress.kubernetes.io/listen-ports='[{"HTTP": 80}]' --overwrite

kubectl annotate ingress echo-server-ingress -n test-apps \
  alb.ingress.kubernetes.io/listen-ports='[{"HTTP": 80}]' --overwrite
```
**Resultado:** ‚ö†Ô∏è Quase - Ainda tinha ssl-redirect annotation

**Tentativa #4 (FINAL):** Remover ssl-redirect
```bash
kubectl annotate ingress nginx-test-ingress -n test-apps \
  alb.ingress.kubernetes.io/ssl-redirect-

kubectl annotate ingress echo-server-ingress -n test-apps \
  alb.ingress.kubernetes.io/ssl-redirect-
```
**Resultado:** ‚úÖ **SUCESSO** - ALBs provisionados em 2-3 minutos

**Valida√ß√£o:**
```bash
kubectl get ingress -n test-apps
# NAME                   CLASS   HOSTS   ADDRESS                                                      PORTS   AGE
# nginx-test-ingress     alb     *       k8s-testapps-nginxtes-bf6521357f-267724084.us-east-1.elb... 80      8m
# echo-server-ingress    alb     *       k8s-testapps-echoserv-d5229efc2b-1385371797.us-east-1.elb.. 80      8m

# Test HTTP responses
curl http://k8s-testapps-nginxtes-bf6521357f-267724084.us-east-1.elb.amazonaws.com
# HTTP/1.1 200 OK
# <html>
# <head><title>Welcome to nginx!</title></head>
# ...

curl http://k8s-testapps-echoserv-d5229efc2b-1385371797.us-east-1.elb.amazonaws.com
# HTTP/1.1 200 OK
# {
#   "host": "k8s-testapps-echoserv-d5229efc2b-1385371797.us-east-1.elb.amazonaws.com",
#   "method": "GET",
#   "path": "/",
#   ...
# }
```

**Lessons Learned:**
- ‚ö†Ô∏è **TLS com ALB requer certificados reais** - N√£o funciona com dom√≠nios fake
- ‚ö†Ô∏è **Cert-Manager + Let's Encrypt requer DNS p√∫blico** - HTTP-01 challenge imposs√≠vel com .local
- ‚ö†Ô∏è **Self-signed certificates precisam configura√ß√£o adequada** - N√£o √© plug-and-play
- ‚úÖ **HTTP-only v√°lido para ambientes de teste** - Aceit√°vel temporariamente
- üìù **Planejar solu√ß√£o TLS adequada** - Route53 + Let's Encrypt OU ACM

---

## ‚úÖ Valida√ß√£o Completa

### 1. Pods Status
```bash
kubectl get pods -n test-apps
# NAME                           READY   STATUS    RESTARTS   AGE
# nginx-test-6d67d58545-bkbgz    2/2     Running   0          12m
# nginx-test-6d67d58545-g6tvh    2/2     Running   0          12m
# echo-server-6987564-7mqfb      1/1     Running   0          10m
# echo-server-6987564-v9xpc      1/1     Running   0          10m

# Verificar node placement
kubectl get pods -n test-apps -o wide
# NODE               NODE-TYPE
# ip-10-0-1-123...   workloads  ‚úÖ
# ip-10-0-2-234...   workloads  ‚úÖ
# ip-10-0-1-124...   workloads  ‚úÖ
# ip-10-0-2-235...   workloads  ‚úÖ
```

‚úÖ **4 pods Running, todos no node group correto**

---

### 2. Services
```bash
kubectl get svc -n test-apps
# NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE
# nginx-test    ClusterIP   172.20.123.45    <none>        80/TCP,9113/TCP   12m
# echo-server   ClusterIP   172.20.234.56    <none>        8080/TCP          12m

# Test internal connectivity
kubectl run -it --rm debug --image=curlimages/curl --restart=Never -- \
  curl http://nginx-test.test-apps.svc.cluster.local
# HTTP/1.1 200 OK
# <html>...</html>

kubectl run -it --rm debug --image=curlimages/curl --restart=Never -- \
  curl http://echo-server.test-apps.svc.cluster.local:8080
# HTTP/1.1 200 OK
# {"host":"echo-server.test-apps.svc.cluster.local:8080",...}
```

‚úÖ **Services acess√≠veis internamente (ClusterIP funcionando)**

---

### 3. Ingresses & ALBs
```bash
kubectl get ingress -n test-apps
# NAME                   CLASS   HOSTS   ADDRESS                                                      PORTS   AGE
# nginx-test-ingress     alb     *       k8s-testapps-nginxtes-bf6521357f-267724084.us-east-1.elb... 80      15m
# echo-server-ingress    alb     *       k8s-testapps-echoserv-d5229efc2b-1385371797.us-east-1.elb.. 80      15m

# AWS CLI validation
aws elbv2 describe-load-balancers \
  --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-testapps`)].{Name:LoadBalancerName,DNS:DNSName,State:State.Code}' \
  --output table
# |  Name                              |  DNS                                             |  State  |
# |  k8s-testapps-nginxtes-bf6521357f  |  k8s-testapps-nginxtes-bf6521357f-267724084...   |  active |
# |  k8s-testapps-echoserv-d5229efc2b  |  k8s-testapps-echoserv-d5229efc2b-1385371797...  |  active |

# Test external HTTP access
curl -I http://k8s-testapps-nginxtes-bf6521357f-267724084.us-east-1.elb.amazonaws.com
# HTTP/1.1 200 OK
# Server: nginx/1.27.3
# Content-Type: text/html

curl -I http://k8s-testapps-echoserv-d5229efc2b-1385371797.us-east-1.elb.amazonaws.com
# HTTP/1.1 200 OK
# Content-Type: application/json
```

‚úÖ **2 ALBs ativos, respondendo HTTP 200**

---

### 4. Network Policies
```bash
kubectl get networkpolicy -n test-apps
# NAME                       POD-SELECTOR   AGE
# allow-alb-and-monitoring   <none>         15m

kubectl describe networkpolicy allow-alb-and-monitoring -n test-apps
# Spec:
#   PodSelector:     <none> (Allowing the specific traffic to all pods in this namespace)
#   Allowing ingress traffic:
#     From NamespaceSelector: name=kube-system
#       Ports: 80/TCP, 8080/TCP, 9113/TCP
#     From NamespaceSelector: name=monitoring
#       Ports: 9113/TCP, 80/TCP
#   Allowing egress traffic:
#     To NamespaceSelector: name=kube-system
#       Ports: 53/UDP (DNS)
#     To any destination (internet access permitido)

# Test: Tr√°fego ALB ‚Üí Pods permitido
kubectl logs -n kube-system deployment/aws-load-balancer-controller | grep -i "test-apps"
# Successfully reconciled ingress: test-apps/nginx-test-ingress
# Successfully reconciled ingress: test-apps/echo-server-ingress
```

‚úÖ **Network Policy permitindo tr√°fego ALB e Prometheus**

---

### 5. Prometheus Integration
```bash
kubectl get servicemonitor -n test-apps
# NAME          AGE
# nginx-test    15m
# echo-server   15m

# Prometheus Operator descobriu?
kubectl logs -n monitoring statefulset/prometheus-kube-prometheus-stack-prometheus | grep -i "servicemonitor"
# level=info msg="Found new ServiceMonitor" namespace=test-apps name=nginx-test
# level=info msg="Found new ServiceMonitor" namespace=test-apps name=echo-server

# Acessar Prometheus UI (port-forward)
kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090

# No browser: http://localhost:9090
# Query: nginx_connections_active
# Resultado: 2 series (1 por pod)
# Labels: {pod="nginx-test-6d67d58545-bkbgz", namespace="test-apps"}
#         {pod="nginx-test-6d67d58545-g6tvh", namespace="test-apps"}

# Targets status
# http://localhost:9090/targets
# test-apps/nginx-test/0 (172.20.123.45:9113) - UP
# test-apps/nginx-test/1 (172.20.234.56:9113) - UP
```

‚úÖ **Prometheus scraping m√©tricas NGINX Exporter (2 targets UP)**

**M√©tricas Dispon√≠veis:**
- `nginx_connections_active` - Conex√µes ativas
- `nginx_connections_accepted` - Total de conex√µes aceitas
- `nginx_connections_handled` - Total de conex√µes tratadas
- `nginx_http_requests_total` - Total de requests HTTP

---

### 6. Loki Logging
```bash
# Grafana port-forward
kubectl port-forward -n monitoring svc/kube-prometheus-stack-grafana 3000:80

# No Grafana: http://localhost:3000
# User: admin
# Password: (AWS Secrets Manager)
aws secretsmanager get-secret-value \
  --secret-id k8s-platform-prod/grafana-admin-password \
  --query SecretString --output text

# Explore ‚Üí Loki
# Query: {namespace="test-apps"}
# Resultado: Logs de todos os 4 pods vis√≠veis
#   - nginx-test-6d67d58545-bkbgz (container: nginx)
#   - nginx-test-6d67d58545-bkbgz (container: nginx-exporter)
#   - nginx-test-6d67d58545-g6tvh (container: nginx)
#   - nginx-test-6d67d58545-g6tvh (container: nginx-exporter)
#   - echo-server-6987564-7mqfb
#   - echo-server-6987564-v9xpc

# Query espec√≠fica: {namespace="test-apps", app="nginx-test"} |= "GET"
# Resultado: Access logs de requests HTTP GET
# Exemplo:
# 172.31.5.123 - - [28/Jan/2026:15:30:45 +0000] "GET / HTTP/1.1" 200 615 "-" "curl/7.68.0"

# Query Echo Server: {namespace="test-apps", app="echo-server"}
# Resultado: JSON response logs
```

‚úÖ **Loki coletando logs de todos os pods (6 streams: 4 nginx containers + 2 echo-server)**

---

## üí∞ An√°lise de Custos

### Recursos Criados e Custos

| Recurso | Quantidade | Custo Mensal | Custo Anual | Nota |
|---------|-----------|--------------|-------------|------|
| **Application Load Balancer** | 2 | $32.40 | $388.80 | $16.20 cada ($0.0225/hora √ó 720h) |
| **ALB Data Processing** | - | ~$8.00 | ~$96.00 | Estimado: 100GB/m√™s √ó $0.008/GB |
| **EC2 Nodes (pods)** | 0 | $0 | $0 | Reutiliza nodes workloads existentes (3 t3.medium) |
| **EBS Volumes (pods)** | 0 | $0 | $0 | Pods sem PVCs (stateless) |
| **Total Fase 7** | - | **$40.40** | **$484.80** | |

### Custo Total Plataforma (Atualizado)

| Marco/Fase | Componente | Custo/M√™s | Acumulado |
|-----------|------------|-----------|-----------|
| Marco 0 | Backend S3+DynamoDB | $0.07 | $0.07 |
| Marco 1 | EKS Control Plane | $73.00 | $73.07 |
| Marco 1 | EC2 Nodes (7 √ó t3.medium) | $477.00 | $550.07 |
| Marco 2 Fase 1 | ALB Controller (software) | $0.00 | $550.07 |
| Marco 2 Fase 2 | Cert-Manager (software) | $0.00 | $550.07 |
| Marco 2 Fase 3 | Prometheus Stack (PVCs + Secrets) | $2.56 | $552.63 |
| Marco 2 Fase 4 | Loki + Fluent Bit (S3 + PVCs) | $19.70 | $572.33 |
| Marco 2 Fase 5 | Network Policies (software) | $0.00 | $572.33 |
| Marco 2 Fase 6 | Cluster Autoscaler (software) | $0.00 | $572.33 |
| **Marco 2 Fase 7** | **Test Applications (2 ALBs)** | **$40.40** | **$612.73** |

### Otimiza√ß√£o de Custos Futura

**Consolidar ALBs usando IngressGroup** (Economia: $16.20/m√™s = $194.40/ano)

Modificar annotations nos Ingresses:
```yaml
annotations:
  alb.ingress.kubernetes.io/group.name: test-apps-shared
  alb.ingress.kubernetes.io/group.order: '10'  # nginx
  # ou '20' para echo-server
```

**Resultado:** 1 ALB compartilhado com m√∫ltiplos targets
- Custo: $16.20/m√™s (em vez de $32.40/m√™s)
- Trade-off: Single point of failure (aceit√°vel para teste)

**Deletar Test Apps Ap√≥s Valida√ß√£o** (Economia: $40.40/m√™s = $484.80/ano)

Ap√≥s Marco 3 (GitLab) estar operacional:
```bash
terraform destroy -target=module.test_applications
# ou
kubectl delete namespace test-apps
```

---

## üìä Checklist de Valida√ß√£o (Final)

### Funcionalidade
- [x] **Namespace criado** - test-apps com labels corretos
- [x] **4 pods Running** - 2 nginx (multi-container), 2 echo-server
- [x] **2 Services ClusterIP** - nginx-test (80, 9113), echo-server (8080)
- [x] **2 Ingresses criados** - ingressClassName: alb
- [x] **2 ALBs provisionados** - internet-facing, target-type: ip
- [x] **HTTP 200 responses** - Ambos ALBs acess√≠veis externamente
- [x] **Network Policy funcionando** - Tr√°fego ALB ‚Üí Pods permitido
- [x] **NodeSelector funcionando** - Pods agendados em workloads nodes

### Observabilidade
- [x] **2 ServiceMonitors criados** - Prometheus auto-discovery
- [x] **Prometheus scraping** - M√©tricas nginx_* vis√≠veis (2 targets UP)
- [x] **Loki logs coletados** - 6 streams (4 nginx containers + 2 echo-server)
- [x] **Grafana Explore** - Query `{namespace="test-apps"}` retorna logs
- [x] **Fluent Bit funcionando** - DaemonSet enviando logs para Loki

### Seguran√ßa
- [x] **Network Policy aplicada** - Isolamento namespace
- [x] **Egress DNS permitido** - Pods podem resolver nomes
- [x] **Ingress seletivo** - Apenas kube-system e monitoring
- [x] **Resources limits** - CPU/Memory definidos (prevent noisy neighbor)
- [ ] ‚ö†Ô∏è **TLS habilitado** - PENDENTE (problema identificado, solu√ß√£o planejada)

### Documenta√ß√£o
- [x] **Terraform module criado** - modules/test-applications/
- [x] **Manifests criados** - nginx-test.yaml, echo-server.yaml
- [x] **kubectl provider configurado** - providers.tf atualizado
- [x] **Integration main.tf** - Module invocation adicionada
- [x] **Validation script** - scripts/validate-fase7.sh (350 linhas)
- [x] **Di√°rio de bordo atualizado** - Entrada Fase 7 completa
- [ ] **ADR-008 criado** - PENDENTE (TLS Strategy - a ser feito)

---

## üö® Issue TLS - An√°lise Detalhada

### Problema
**ALBs n√£o foram provisionados inicialmente devido a configura√ß√£o incorreta de TLS.**

### Root Causes

**1. Dom√≠nios Fake (.local) sem DNS Real**
- Ingresses configurados com `tls.hosts: ["nginx-test.test-apps.local"]`
- Dom√≠nios `.local` s√£o internos (n√£o resolvem publicamente)
- Cert-Manager n√£o consegue validar dom√≠nios sem DNS p√∫blico

**2. Let's Encrypt HTTP-01 Challenge Imposs√≠vel**
- Let's Encrypt (staging/production issuers) usa HTTP-01 challenge
- Requer: `http://nginx-test.test-apps.local/.well-known/acme-challenge/TOKEN`
- Dom√≠nio fake n√£o resolve ‚Üí Challenge falha
- Certificado n√£o emitido

**3. Self-signed Issuer Mal Configurado**
- Annotation `cert-manager.io/cluster-issuer: selfsigned-issuer` presente
- Self-signed issuer teve optimistic locking issues (bug/config problem)
- Certificate resources stuck em "Ready: False"

**4. ALB Controller Bloqueia HTTPS sem Certificados**
- Quando `spec.tls` est√° presente no Ingress, ALB Controller espera certificados reais
- Annotation `listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'` ativa HTTPS listener
- AWS ALB API erro: `ValidationError: A certificate must be specified for HTTPS listeners`
- ALB Controller n√£o prosseguiu (bloqueou cria√ß√£o do ALB inteiro)

### Timeline do Problema

1. **T+0min:** `terraform apply` cria Ingresses com TLS section
2. **T+1min:** Cert-Manager detecta annotation, cria Certificate resources
3. **T+2min:** Certificates stuck em "Ready: False" (validation failure)
4. **T+3min:** ALB Controller detecta TLS, aguarda certificados
5. **T+5min:** ALB Controller tenta criar ALB, AWS API retorna erro
6. **T+8min:** Ingresses sem ADDRESS (ALB n√£o provisionado)
7. **T+10min:** Troubleshooting iniciado (logs ALB Controller, Certificate describe)
8. **T+15min:** Removido TLS section via kubectl patch
9. **T+18min:** Alterado listen-ports para HTTP-only
10. **T+20min:** ALBs provisionados com sucesso (HTTP 200)

### Solu√ß√µes Testadas (Tentativas)

| Tentativa | A√ß√£o | Resultado | Motivo |
|-----------|------|-----------|--------|
| #1 | Remover annotation `cert-manager.io/cluster-issuer` | ‚ùå Falhou | ALB Controller ainda via `spec.tls` |
| #2 | Remover `spec.tls` section via kubectl patch | ‚ö†Ô∏è Parcial | ALB criou LB mas tentou HTTPS listener |
| #3 | Alterar `listen-ports` para `[{"HTTP": 80}]` | ‚ö†Ô∏è Quase | Ainda tinha `ssl-redirect: "443"` annotation |
| #4 (FINAL) | Remover `ssl-redirect` annotation | ‚úÖ **SUCESSO** | ALBs provisionados HTTP-only |

### Solu√ß√£o Aplicada (Tempor√°ria)

**Configura√ß√£o HTTP-Only:**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
    alb.ingress.kubernetes.io/healthcheck-path: /
    # REMOVIDO: cert-manager.io/cluster-issuer: selfsigned-issuer
    # REMOVIDO: alb.ingress.kubernetes.io/ssl-redirect: "443"
spec:
  ingressClassName: alb
  # REMOVIDO: tls section
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-test
            port:
              number: 80
```

**Impacto:**
- ‚úÖ ALBs provisionados e operacionais
- ‚úÖ Valida√ß√£o end-to-end funcional (Ingress ‚Üí ALB ‚Üí Pods ‚Üí Prometheus ‚Üí Loki)
- ‚ö†Ô∏è Tr√°fego HTTP n√£o criptografado (aceit√°vel para ambiente de teste)
- ‚ö†Ô∏è Cert-Manager n√£o validado em cen√°rio real
- üìù Necess√°rio planejar solu√ß√£o TLS adequada para produ√ß√£o

---

## üîÆ Pr√≥ximas Solu√ß√µes TLS (Planejadas)

### Op√ß√£o A: Route53 + Let's Encrypt (HTTP-01 Challenge)

**Requisitos:**
- Dom√≠nio real registrado (ex: `k8s-platform.example.com`)
- Zona hospedada no Route53 (ou DNS externo)
- Subdomain delegation: `*.test.k8s-platform.example.com` ‚Üí Route53

**Configura√ß√£o:**
```yaml
# ClusterIssuer (letsencrypt-production j√° existe)
# Ingress atualizado
spec:
  tls:
  - hosts:
    - nginx.test.k8s-platform.example.com
    secretName: nginx-tls
  rules:
  - host: nginx.test.k8s-platform.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-test
            port:
              number: 80
```

**Annotations:**
```yaml
annotations:
  cert-manager.io/cluster-issuer: letsencrypt-production
  alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'
  alb.ingress.kubernetes.io/ssl-redirect: '443'
  alb.ingress.kubernetes.io/certificate-arn: # (auto-discovery via TLS secret)
```

**Pros:**
- ‚úÖ Certificados v√°lidos (Let's Encrypt trusted CA)
- ‚úÖ Auto-renewal (Cert-Manager)
- ‚úÖ Zero custo (Let's Encrypt free)

**Cons:**
- ‚ö†Ô∏è Requer dom√≠nio real ($12/ano)
- ‚ö†Ô∏è Route53 hosted zone ($0.50/m√™s)
- ‚ö†Ô∏è DNS propagation delay (at√© 48h)

**Custo:** ~$18/ano (dom√≠nio $12 + Route53 $6)

---

### Op√ß√£o B: AWS Certificate Manager (ACM) + Dom√≠nio Real

**Requisitos:**
- Dom√≠nio real em Route53 (ou DNS externo com valida√ß√£o email)
- Certificado ACM criado manualmente
- ALB annotation com ARN do certificado

**Steps:**
```bash
# 1. Criar certificado ACM (console ou CLI)
aws acm request-certificate \
  --domain-name "*.test.k8s-platform.example.com" \
  --validation-method DNS \
  --subject-alternative-names "test.k8s-platform.example.com"

# 2. Validar certificado (Route53 automation ou manual CNAME)
# AWS console: Certificate Manager ‚Üí Pending validation ‚Üí Create records in Route53

# 3. Obter ARN do certificado
aws acm list-certificates --query 'CertificateSummaryList[?DomainName==`*.test.k8s-platform.example.com`].CertificateArn' --output text
# arn:aws:acm:us-east-1:891377105802:certificate/abc123...
```

**Ingress Configuration:**
```yaml
annotations:
  alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:891377105802:certificate/abc123...
  alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'
  alb.ingress.kubernetes.io/ssl-redirect: '443'
  # N√ÉO usar: cert-manager.io/cluster-issuer (ACM gerencia cert)

spec:
  tls:
  - hosts:
    - nginx.test.k8s-platform.example.com
    # secretName N√ÉO necess√°rio (ACM cert diretamente no ALB)
```

**Pros:**
- ‚úÖ Certificados gratuitos (ACM free)
- ‚úÖ Auto-renewal (ACM managed)
- ‚úÖ Integra√ß√£o nativa AWS (menos moving parts)
- ‚úÖ Wildcard support (`*.test.k8s-platform.example.com`)

**Cons:**
- ‚ö†Ô∏è Requer dom√≠nio real ($12/ano)
- ‚ö†Ô∏è Vendor lock-in (ACM s√≥ funciona com AWS services)
- ‚ö†Ô∏è Cert-Manager bypass (n√£o usa Kubernetes-native approach)

**Custo:** $12/ano (apenas dom√≠nio)

---

### Op√ß√£o C: Self-signed Certificates (Apenas Dev/Test)

**Requisitos:**
- Nenhum dom√≠nio real necess√°rio
- Cert-Manager self-signed issuer configurado corretamente
- Aceitar browser warnings (untrusted CA)

**ClusterIssuer Configuration:**
```yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-issuer-fixed
spec:
  selfSigned: {}
```

**Ingress Configuration:**
```yaml
annotations:
  cert-manager.io/cluster-issuer: selfsigned-issuer-fixed
  alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'
  # Sem ssl-redirect (permitir HTTP para troubleshooting)

spec:
  tls:
  - hosts:
    - nginx-test.test-apps.local  # Qualquer nome (n√£o precisa resolver)
    secretName: nginx-test-selfsigned-tls
```

**Troubleshooting Self-signed:**
```bash
# Verificar Certificate creation
kubectl get certificate -n test-apps
# NAME                      READY   SECRET                      AGE
# nginx-test-selfsigned-tls True    nginx-test-selfsigned-tls   5m

# Verificar Secret criado
kubectl get secret nginx-test-selfsigned-tls -n test-apps
# NAME                      TYPE                DATA   AGE
# nginx-test-selfsigned-tls kubernetes.io/tls   2      5m

# Validar certificado
kubectl get secret nginx-test-selfsigned-tls -n test-apps -o jsonpath='{.data.tls\.crt}' | base64 -d | openssl x509 -noout -text
# Issuer: CN=nginx-test.test-apps.local
# Subject: CN=nginx-test.test-apps.local
# Validity: 90 days
```

**Pros:**
- ‚úÖ Zero custo (sem dom√≠nio, sem DNS)
- ‚úÖ R√°pido (sem valida√ß√£o externa)
- ‚úÖ Kubernetes-native (Cert-Manager)

**Cons:**
- ‚ùå Browser warnings (untrusted CA)
- ‚ùå N√£o v√°lido para produ√ß√£o
- ‚ö†Ô∏è Self-signed issuer teve issues (necess√°rio fix)

**Uso:** Apenas dev/test local

---

### Op√ß√£o D: Let's Encrypt DNS-01 Challenge (Sem ALB P√∫blico)

**Requisitos:**
- Route53 hosted zone
- Cert-Manager com Route53 solver
- IRSA para Cert-Manager (IAM permissions Route53)

**ClusterIssuer Configuration:**
```yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-dns01
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: devops@example.com
    privateKeySecretRef:
      name: letsencrypt-dns01-private-key
    solvers:
    - dns01:
        route53:
          region: us-east-1
          # IRSA: ServiceAccount annotation
          # eks.amazonaws.com/role-arn: arn:aws:iam::891377105802:role/CertManagerRoute53Role
```

**Pros:**
- ‚úÖ Funciona com ALB interno (n√£o precisa ser internet-facing)
- ‚úÖ Wildcard certificates (`*.test.k8s-platform.example.com`)
- ‚úÖ Trusted CA (Let's Encrypt)

**Cons:**
- ‚ö†Ô∏è Complexidade (IRSA, IAM policy Route53)
- ‚ö†Ô∏è Requer dom√≠nio + Route53
- ‚ö†Ô∏è Mais lento que HTTP-01 (DNS propagation)

**Custo:** ~$6/ano (Route53 hosted zone, dom√≠nio √† parte)

---

### Compara√ß√£o de Solu√ß√µes TLS

| Solu√ß√£o | Custo/Ano | Complexidade | Prod-Ready | Requer Dom√≠nio | Auto-Renewal |
|---------|-----------|--------------|------------|----------------|--------------|
| **Route53 + Let's Encrypt HTTP-01** | $18 | M√©dia | ‚úÖ Sim | ‚úÖ Sim | ‚úÖ Sim |
| **ACM + Dom√≠nio** | $12 | Baixa | ‚úÖ Sim | ‚úÖ Sim | ‚úÖ Sim (ACM) |
| **Self-signed** | $0 | Baixa | ‚ùå N√£o | ‚ùå N√£o | ‚úÖ Sim (90d) |
| **Let's Encrypt DNS-01** | $6 + dom√≠nio | Alta | ‚úÖ Sim | ‚úÖ Sim | ‚úÖ Sim |
| **HTTP-only (atual)** | $0 | Muito Baixa | ‚ùå N√£o | ‚ùå N√£o | N/A |

**Recomenda√ß√£o:**
- **Curto prazo (Fase 7 valida√ß√£o):** Manter HTTP-only ‚úÖ (j√° funcional)
- **M√©dio prazo (Marco 3 workloads):** **ACM + Dom√≠nio** (simplicidade, custo baixo)
- **Longo prazo (produ√ß√£o):** Route53 + Let's Encrypt HTTP-01 (portabilidade, Kubernetes-native)

---

## üìö Lessons Learned

### T√©cnicas

1. **kubectl Terraform Provider ‚úÖ**
   - Excelente para aplicar manifests Kubernetes via Terraform
   - Suporta CRDs nativamente (ServiceMonitor, Certificate)
   - Melhor que `kubernetes_manifest` (Hashicorp provider) para multi-document YAML
   - `for_each` pattern clean: `data.kubectl_file_documents.nginx_test.manifests`

2. **AWS Load Balancer Controller ‚úÖ**
   - Funciona perfeitamente com `target-type: ip` + Network Policies
   - Auto-provisioning de ALBs r√°pido (~2-3 min ap√≥s Ingress creation)
   - Requer certificados reais quando TLS section presente no Ingress
   - IngressGroup annotation permite compartilhar ALB (economia)

3. **Sidecar Pattern (NGINX + Exporter) ‚úÖ**
   - Multi-container pods funcionam bem
   - NGINX Exporter em sidecar: zero config Prometheus metrics
   - Resources limits importantes (prevent noisy neighbor)
   - `localhost` communication entre containers no mesmo pod

4. **Prometheus ServiceMonitor ‚úÖ**
   - Auto-discovery funciona perfeitamente
   - Label `release: kube-prometheus-stack` essencial para discovery
   - Zero configuration Prometheus (operator handles everything)
   - Metrics vis√≠veis em ~30s ap√≥s pod start

5. **Loki + Fluent Bit ‚úÖ**
   - DaemonSet pattern eficaz (1 pod por node, coleta todos logs)
   - Zero config necess√°ria (Fluent Bit auto-detecta namespaces)
   - Query LogQL funciona perfeitamente: `{namespace="test-apps"}`
   - Multi-container logs separados em streams

### Problemas e Solu√ß√µes

6. **TLS com Dom√≠nios Fake ‚ùå**
   - **Problema:** ALB Controller bloqueia cria√ß√£o sem certificados reais
   - **Root Cause:** Dom√≠nios `.local` n√£o resolvem publicamente, Cert-Manager n√£o valida
   - **Li√ß√£o:** TLS requer DNS real OU certificados ACM pre-existentes OU self-signed corretamente configurado
   - **Solu√ß√£o:** HTTP-only temporariamente, planejar TLS adequado (ACM ou Route53)

7. **ImagePullBackOff (Vers√£o Inexistente) ‚ö†Ô∏è**
   - **Problema:** `ealen/echo-server:0.9.4` n√£o existe no Docker Hub
   - **Li√ß√£o:** Sempre validar image existence antes de deploy (docker pull ou registry API)
   - **Solu√ß√£o:** Usar `latest` tag OU fixar vers√£o conhecida (e.g., `0.8.5`)

8. **WSL DNS Resolver Failure ‚ö†Ô∏è**
   - **Problema:** WSL2 DNS (10.255.255.254) n√£o respondendo, bloqueou AWS API calls
   - **Li√ß√£o:** WSL pode ter DNS issues intermitentes (VPN, network changes)
   - **Solu√ß√£o:** Configurar Google DNS (8.8.8.8) manualmente + disable auto-generation

### Arquiteturais

9. **Network Policies com ALB ‚úÖ**
   - Ingress de `kube-system` namespace permite tr√°fego ALB ‚Üí Pods
   - `target-type: ip` essencial (ALB acessa pods diretamente via ENI, n√£o NodePort)
   - Egress para internet necess√°rio (pods podem precisar acessar APIs externas)

10. **NodeSelector para Workloads ‚úÖ**
    - Test apps agendados corretamente em node group `workloads`
    - System pods (Prometheus, Loki) em node group `system`
    - Separa√ß√£o permite Cluster Autoscaler escalar apenas workloads (economia)

11. **Consolida√ß√£o de Custos üí∞**
    - IngressGroup annotation permite compartilhar ALB entre m√∫ltiplos Ingresses
    - Economia: $16.20/m√™s (1 ALB em vez de 2)
    - Trade-off: Single point of failure (aceit√°vel para teste)

### Operacionais

12. **Troubleshooting Ingress Issues üîç**
    - **Passo 1:** `kubectl get ingress` - Verificar ADDRESS presente
    - **Passo 2:** `kubectl logs -n kube-system deployment/aws-load-balancer-controller` - Checar erros ALB Controller
    - **Passo 3:** `kubectl describe ingress` - Verificar events e annotations
    - **Passo 4:** `aws elbv2 describe-load-balancers` - Confirmar ALB criado na AWS
    - **Passo 5:** `kubectl get certificate` - Verificar status de TLS (se aplic√°vel)

13. **Valida√ß√£o End-to-End Checklist ‚úÖ**
    - Pods Running ‚Üí Services acess√≠veis ‚Üí Ingresses com ADDRESS ‚Üí ALBs respondendo HTTP 200
    - Prometheus scraping ‚Üí Loki logs vis√≠veis ‚Üí Network Policies permitindo tr√°fego

14. **Documentation-Driven Development üìù**
    - Criar documenta√ß√£o durante implementa√ß√£o (n√£o depois)
    - Script de valida√ß√£o essencial (reproduce validation steps)
    - Di√°rio de bordo invaluable para troubleshooting futuro

---

## üéØ Pr√≥ximos Passos

### Imediato (Fase 7 - Continua√ß√£o)
1. [ ] **Analisar solu√ß√µes TLS** usando [executor-terraform.md](../../../../../docs/prompts/executor-terraform.md) framework
2. [ ] **Criar ADR-008:** TLS Strategy - Decis√£o Route53 vs ACM vs Self-signed
3. [ ] **Implementar solu√ß√£o TLS escolhida**
4. [ ] **Atualizar Ingresses** com TLS habilitado
5. [ ] **Validar HTTPS** (curl -k, browser, certificado v√°lido)

### Curto Prazo (Otimiza√ß√µes - 1-2 semanas)
6. [ ] **Consolidar ALBs** - IngressGroup annotation (economia $16.20/m√™s)
7. [ ] **Testar auto-scaling** - Gerar carga nginx para trigger Cluster Autoscaler scale-up
8. [ ] **Dashboard Grafana** - Criar dashboard espec√≠fico (NGINX + Echo Server metrics)
9. [ ] **Alertas Prometheus** - Notificar se ALB healthcheck fail (PagerDuty ou email)

### Marco 3 (Workloads Produtivos - 1-3 meses)
10. [ ] **GitLab CE deployment** - CI/CD platform (RDS PostgreSQL, S3 artifacts, Redis)
11. [ ] **Keycloak** - Identity & Access Management (SSO, OAuth2/OIDC)
12. [ ] **ArgoCD** - GitOps continuous delivery (sync com GitLab repos)
13. [ ] **Harbor** - Container registry (S3 backend, Trivy scanning)
14. [ ] **SonarQube** - Code quality analysis (RDS PostgreSQL)

### Decommission (Ap√≥s Marco 3 Operacional)
15. [ ] **Deletar test apps** - `terraform destroy -target=module.test_applications` (economia $40.40/m√™s)
16. [ ] **Review custos** - AWS Cost Explorer, identificar waste
17. [ ] **Reserved Instances** - Avaliar RI para EC2 nodes (economia 31%)

---

## üìñ Refer√™ncias

### Documenta√ß√£o Oficial
- [AWS Load Balancer Controller](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.11/)
- [Cert-Manager Documentation](https://cert-manager.io/docs/)
- [Prometheus Operator ServiceMonitor](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/getting-started.md)
- [Fluent Bit Kubernetes Filter](https://docs.fluentbit.io/manual/pipeline/filters/kubernetes)
- [Kubernetes Network Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)

### ADRs Relevantes
- [ADR-003: Secrets Management Strategy](../../../../../docs/adr/adr-003-secrets-management-strategy.md)
- [ADR-004: Terraform vs Helm for Platform Services](../../../../../docs/adr/adr-004-terraform-vs-helm.md)
- [ADR-005: Logging Strategy](../../../../../docs/adr/adr-005-logging-strategy.md)
- [ADR-006: Network Policies Strategy](../../../../../docs/adr/adr-006-network-policies-strategy.md)
- [ADR-007: Cluster Autoscaler Strategy](../../../../../docs/adr/adr-007-cluster-autoscaler-strategy.md)
- **ADR-008: TLS Strategy** - A SER CRIADO

### Contexto do Projeto
- [Executor Terraform Framework](../../../../../docs/prompts/executor-terraform.md)
- [AWS Console Execution Plan](../../../../../docs/plan/aws-console-execution-plan.md)
- [Di√°rio de Bordo](../../../../../docs/plan/aws-execution/00-diario-de-bordo.md)
- [Observability Stack Plan](../../../../../docs/plan/aws-execution/04-observability-stack.md)

---

**Status:** ‚úÖ FASE 7 COMPLETO (HTTP-only)
**Pr√≥ximo:** An√°lise TLS + ADR-008 + Implementa√ß√£o HTTPS
**Data:** 2026-01-28
**Executado por:** DevOps Team + Claude Sonnet 4.5
