# âš ï¸ AnÃ¡lise de Riscos - Plataforma Kubernetes AWS

**Ãšltima AtualizaÃ§Ã£o:** 2026-01-29
**VersÃ£o:** 2.0 (Marco 2 Completo)
**Framework:** Baseado em executor-terraform.md

---

## ğŸ“Š Matriz de Riscos

| ID | Risco | Probabilidade | Impacto | Severidade | Status | MitigaÃ§Ã£o |
|----|-------|---------------|---------|------------|--------|-----------|
| R-001 | State lock travado | BAIXO | MÃ‰DIO | ğŸŸ¡ MÃ‰DIO | âœ… Mitigado | DynamoDB locking + force-unlock |
| R-002 | EKS add-ons deadlock | BAIXO | ALTO | ğŸŸ¡ MÃ‰DIO | âœ… Resolvido | Dependency order fixado |
| R-003 | Network Policies bloqueiam trÃ¡fego | MÃ‰DIO | ALTO | ğŸ”´ ALTO | âœ… Mitigado | Mapeamento de fluxos prÃ©vio |
| R-004 | Custos S3 Loki excedem estimativa | MÃ‰DIO | BAIXO | ğŸŸ¢ BAIXO | âš ï¸ Monitorar | CloudWatch billing alerts |
| R-005 | ACM certificate expiration | BAIXO | MÃ‰DIO | ğŸŸ¡ MÃ‰DIO | âœ… Mitigado | Auto-renewal ACM + alarm |
| R-006 | ALB provisioning timeout | BAIXO | MÃ‰DIO | ğŸŸ¡ MÃ‰DIO | âœ… Tolerado | Retry terraform apply |
| R-007 | Pods OOMKilled (memory limits) | MÃ‰DIO | MÃ‰DIO | ğŸŸ¡ MÃ‰DIO | âš ï¸ Monitorar | Prometheus alerts + tuning |
| R-008 | Vendor lock-in AWS | ALTO | BAIXO | ğŸŸ¡ MÃ‰DIO | âœ… Aceito | Trade-off custo vs portabilidade |
| R-009 | Single AZ failure (2 AZs only) | BAIXO | ALTO | ğŸŸ¡ MÃ‰DIO | âœ… Aceito | RTO 15min (recreate nodes) |
| R-010 | Secrets leak em Git | BAIXO | CRÃTICO | ğŸ”´ ALTO | âœ… Mitigado | AWS Secrets Manager + pre-commit hooks |
| R-011 | Drift entre Terraform state e recursos | MÃ‰DIO | MÃ‰DIO | ğŸŸ¡ MÃ‰DIO | âœ… Mitigado | Terraform plan daily + drift detection |
| R-012 | Cluster Autoscaler scale-down agressivo | BAIXO | MÃ‰DIO | ğŸŸ¡ MÃ‰DIO | âœ… Mitigado | 5min threshold + PDB configurados |

---

## ğŸ”´ Riscos CrÃ­ticos (ALTO Impacto)

### R-010: Secrets Leak em Git

**Probabilidade:** BAIXO
**Impacto:** CRÃTICO
**Severidade:** ğŸ”´ ALTO

**DescriÃ§Ã£o:**
Credenciais sensÃ­veis (Grafana password, API keys, database credentials) commitadas acidentalmente em repositÃ³rio Git.

**CenÃ¡rio de Falha:**
1. Desenvolvedor adiciona secret em `terraform.tfvars`
2. Commit sem validaÃ§Ã£o
3. Push para repositÃ³rio (GitHub public/private)
4. Credenciais expostas (scan automÃ¡tico de bots)
5. Acesso nÃ£o autorizado a plataforma

**MitigaÃ§Ãµes Implementadas:**
- âœ… **AWS Secrets Manager:** Todas credenciais sensÃ­veis armazenadas externamente
- âœ… **Pre-commit hook:** ValidaÃ§Ã£o automÃ¡tica (block commits com secrets)
- âœ… **.gitignore:** terraform.tfvars, *.tfvars, *.env (ignored)
- âœ… **Governance validation:** Hook custom validando padrÃµes de secrets

**MitigaÃ§Ãµes Adicionais (Futuro):**
- [ ] **git-secrets AWS plugin:** Scan automÃ¡tico de padrÃµes AWS (Access Keys, etc)
- [ ] **Periodic audit:** Monthly review de commits histÃ³ricos
- [ ] **Rotation policy:** Quarterly rotation de credentials (Secrets Manager)

**Monitoramento:**
- CloudTrail: Unauthorized API calls (DetectPortScan rule)
- GuardDuty: Credential compromise detection (se habilitado)

---

### R-003: Network Policies Bloqueiam TrÃ¡fego Essencial

**Probabilidade:** MÃ‰DIO
**Impacto:** ALTO
**Severidade:** ğŸ”´ ALTO

**DescriÃ§Ã£o:**
Network Policies excessivamente restritivas bloqueiam comunicaÃ§Ã£o necessÃ¡ria entre pods, causando falhas em aplicaÃ§Ãµes.

**CenÃ¡rio de Falha:**
1. Deploy nova aplicaÃ§Ã£o no namespace `monitoring`
2. Default-deny policy aplicada
3. App nÃ£o consegue resolver DNS (bloqueado)
4. App nÃ£o consegue acessar API Kubernetes (bloqueado)
5. Pods crashloop, aplicaÃ§Ã£o indisponÃ­vel

**MitigaÃ§Ãµes Implementadas:**
- âœ… **Mapeamento de fluxos:** AnÃ¡lise prÃ©via de comunicaÃ§Ã£o (Prometheus â†’ targets, Fluent Bit â†’ Loki)
- âœ… **Allow-list explÃ­cito:** PolÃ­ticas granulares (DNS, API server, scraping, logging)
- âœ… **Testing em staging:** ValidaÃ§Ã£o antes de aplicar em namespaces produtivos
- âœ… **Rollback rÃ¡pido:** `kubectl delete networkpolicy <name>` (restaura comunicaÃ§Ã£o)

**Monitoramento:**
- Prometheus alerts: Pod restarts > 3 (indica crash loops)
- Logs Loki: Query `{namespace="monitoring"} |= "connection refused"` (detecta bloqueios)

**PrÃ³ximas AÃ§Ãµes:**
- [ ] Criar Grafana dashboard especÃ­fico para Network Policies violations
- [ ] Implementar Calico Policy Audit Mode (log violations sem bloquear)

---

### R-009: Single AZ Failure (Cluster com 2 AZs)

**Probabilidade:** BAIXO
**Impacto:** ALTO
**Severidade:** ğŸŸ¡ MÃ‰DIO

**DescriÃ§Ã£o:**
Falha completa de uma Availability Zone (us-east-1a ou us-east-1b), reduzindo capacidade do cluster pela metade.

**CenÃ¡rio de Falha:**
1. AWS degrada us-east-1a (power outage, network issue)
2. 50% dos nodes ficam unreachable (3-4 nodes perdidos)
3. Pods em nodes afetados entram Terminating
4. Cluster reduz para ~50% capacidade
5. Workloads critical podem falhar se nÃ£o houver rÃ©plicas em AZ saudÃ¡vel

**MitigaÃ§Ãµes Implementadas:**
- âœ… **2 AZs (nÃ£o 1):** Reduz risco de total outage (single AZ seria 100% loss)
- âœ… **Pod Anti-affinity:** RÃ©plicas distribuÃ­das entre AZs diferentes (topology spread)
- âœ… **Cluster Autoscaler:** Auto-scale em AZ saudÃ¡vel (replenish capacity)

**MitigaÃ§Ãµes Aceitas (Trade-off):**
- âš ï¸ **NÃ£o implementar 3 AZs:** Custo adicional $500/mÃªs (33% mais nodes + NAT Gateway)
- âš ï¸ **RTO 15 minutos:** Tempo para Cluster Autoscaler provisionar nodes de reposiÃ§Ã£o

**Monitoramento:**
- AWS Health Dashboard: AZ status (us-east-1 region)
- Prometheus: Node readiness por AZ (alert se >50% nodes down)

**DecisÃ£o:**
âœ… **ACEITO** - RTO 15min aceitÃ¡vel para DevOps workloads (nÃ£o critical user-facing)

---

## ğŸŸ¡ Riscos MÃ©dios

### R-001: State Lock Travado (Terraform)

**Probabilidade:** BAIXO
**Impacto:** MÃ‰DIO
**Severidade:** ğŸŸ¡ MÃ‰DIO

**DescriÃ§Ã£o:**
Lock DynamoDB travado apÃ³s terraform apply falhar ou interrupÃ§Ã£o manual (Ctrl+C).

**CenÃ¡rio de Falha:**
1. `terraform apply` iniciado
2. UsuÃ¡rio interrompe (Ctrl+C) durante apply
3. Lock nÃ£o Ã© liberado automaticamente
4. PrÃ³ximo `terraform plan` falha: "Error acquiring state lock"

**MitigaÃ§Ãµes:**
- âœ… **DynamoDB locking:** Backend S3 com state locking habilitado
- âœ… **Force-unlock command:** `terraform force-unlock <LOCK_ID>` (manual recovery)
- âœ… **Timeout configuration:** 30 segundos timeout para lock acquisition

**ResoluÃ§Ã£o:**
```bash
# 1. Verificar lock ativo
terraform plan
# Error: ID: 78557c8a-c29b-856a-d1d2-ac4df7306c04

# 2. Force unlock (apÃ³s confirmar que nenhum terraform estÃ¡ rodando)
terraform force-unlock -force 78557c8a-c29b-856a-d1d2-ac4df7306c04

# 3. Retry plan
terraform plan
```

**LiÃ§Ãµes Aprendidas:**
- Sempre verificar `ps aux | grep terraform` antes de force-unlock
- Nunca interromper terraform apply (aguardar conclusÃ£o ou usar -auto-approve=false)

---

### R-002: EKS Add-ons Deadlock (RESOLVIDO)

**Probabilidade:** BAIXO (apÃ³s correÃ§Ã£o)
**Impacto:** ALTO
**Severidade:** ğŸŸ¡ MÃ‰DIO
**Status:** âœ… Resolvido (2026-01-28)

**DescriÃ§Ã£o:**
Add-ons EKS entravam em deadlock durante criaÃ§Ã£o paralela, travando deploy do cluster.

**CenÃ¡rio de Falha (Antes da CorreÃ§Ã£o):**
1. Terraform cria 4 add-ons em paralelo (vpc-cni, kube-proxy, coredns, ebs-csi-driver)
2. coredns depende de vpc-cni (networking)
3. CriaÃ§Ã£o paralela causa race condition
4. Add-ons ficam "Degraded" indefinidamente

**CorreÃ§Ã£o Implementada:**
```hcl
# Ordem correta de dependÃªncias:
1. vpc-cni (primeiro - base networking)
2. kube-proxy (depende de vpc-cni)
3. coredns (depende de vpc-cni)
4. ebs-csi-driver (Ãºltimo - storage)
```

**Resultado:**
- âœ… 4/4 add-ons "Active" em ~5 minutos
- âœ… Cluster operacional com 7 nodes
- âœ… Zero degradation

**Arquivo:** `marco1/main.tf` - Dependency order explÃ­cito com `depends_on`

---

### R-004: Custos S3 Loki Excedem Estimativa

**Probabilidade:** MÃ‰DIO
**Impacto:** BAIXO
**Severidade:** ğŸŸ¢ BAIXO

**DescriÃ§Ã£o:**
Uso de S3 para logs Loki excede estimativa inicial ($11.50/mÃªs), devido a maior volume de logs ou retention inadequada.

**CenÃ¡rio de Falha:**
1. AplicaÃ§Ãµes geram logs excessivos (debug level em produÃ§Ã£o)
2. S3 storage cresce para 1TB+ (estimativa era 500GB)
3. Custo S3 duplica: $23/mÃªs (vs $11.50 estimado)
4. Alerta de billing AWS dispara

**MitigaÃ§Ãµes:**
- âœ… **S3 Lifecycle Policy:** 30 dias retention configurado
- âš ï¸ **CloudWatch Billing Alert:** Configurar alarm para S3 > $15/mÃªs (PENDENTE)
- âœ… **Loki Retention:** 7 dias in-memory cache (reduz queries S3)

**PrÃ³ximas AÃ§Ãµes:**
- [ ] Implementar S3 Lifecycle para Glacier apÃ³s 90 dias (80% economia)
- [ ] Configurar CloudWatch billing alerts por serviÃ§o
- [ ] Revisar log levels em aplicaÃ§Ãµes (INFO em prod, nÃ£o DEBUG)

**Monitoramento:**
```bash
# Check S3 storage usage
aws s3 ls s3://k8s-platform-loki-891377105802 --recursive --human-readable --summarize

# Check billing
aws ce get-cost-and-usage --time-period Start=2026-01-01,End=2026-01-31 \
  --granularity MONTHLY --metrics "UnblendedCost" \
  --filter file://s3-filter.json
```

---

### R-007: Pods OOMKilled (Memory Limits Inadequados)

**Probabilidade:** MÃ‰DIO
**Impacto:** MÃ‰DIO
**Severidade:** ğŸŸ¡ MÃ‰DIO

**DescriÃ§Ã£o:**
Pods excedendo memory limits configurados, sendo killed pelo Kubernetes OOM (Out of Memory) killer.

**CenÃ¡rio de Falha:**
1. Prometheus pod configurado com `limits.memory: 2Gi`
2. Workload aumenta (mais targets scraped)
3. Prometheus memory usage atinge 2Gi
4. Kubernetes OOMKills o pod
5. Pod reinicia, perde dados in-memory (queries recentes)

**MitigaÃ§Ãµes:**
- âœ… **Requests < Limits:** Configurado (ex: requests 1Gi, limits 2Gi) permite burst
- âœ… **Prometheus alerts:** Alert `container_memory_usage_bytes` > 80% limit
- âš ï¸ **Tuning periÃ³dico:** Revisar limits baseado em usage histÃ³rico (MANUAL)

**Monitoramento:**
```promql
# Prometheus query para detectar pods prÃ³ximos do OOM
container_memory_working_set_bytes{namespace="monitoring"}
  / container_spec_memory_limit_bytes{namespace="monitoring"}
  > 0.8
```

**AÃ§Ãµes Corretivas:**
- Aumentar limits gradualmente (10-20% por iteraÃ§Ã£o)
- Analisar memory leaks (Grafana heap analysis)
- Considerar horizontal scaling (mais rÃ©plicas, menos memory por pod)

---

## ğŸŸ¢ Riscos Baixos (Aceitos)

### R-008: Vendor Lock-in AWS

**Probabilidade:** ALTO (jÃ¡ estamos locked-in)
**Impacto:** BAIXO (trade-off consciente)
**Severidade:** ğŸŸ¡ MÃ‰DIO
**Status:** âœ… Aceito

**DescriÃ§Ã£o:**
DependÃªncia de serviÃ§os especÃ­ficos AWS (EKS, ALB, S3, ACM, Secrets Manager) dificulta migraÃ§Ã£o para outra cloud.

**Trade-off Consciente:**
- **Vantagem:** Time-to-market 3Ã— faster, custo 50% menor (vs multi-cloud)
- **Desvantagem:** MigraÃ§Ã£o futura para GCP/Azure custaria 200-300h de refactoring

**DecisÃ£o:**
âœ… **ACEITO** - Prioridade atual: Custo + Velocidade > Portabilidade

**MitigaÃ§Ãµes (Arquiteturais):**
- âœ… Loki (cloud-agnostic, poderia usar GCS ou Azure Blob)
- âœ… Prometheus (cloud-agnostic)
- âœ… Calico Network Policies (funciona em qualquer Kubernetes)
- âš ï¸ ALB Controller (AWS-specific) - alternativa: NGINX Ingress Controller

**Reversibilidade:**
- MigraÃ§Ã£o estimada: 200-300h engineering effort
- Ferramentas: Terraform multi-cloud modules, Kubernetes portÃ¡vel (exceto ALB)

---

### R-006: ALB Provisioning Timeout

**Probabilidade:** BAIXO
**Impacto:** MÃ‰DIO
**Severidade:** ğŸŸ¡ MÃ‰DIO
**Status:** âœ… Tolerado

**DescriÃ§Ã£o:**
ALB provisioning via Ingress demora 3-5 minutos, ocasionalmente timeout 10min.

**CenÃ¡rio:**
1. `kubectl apply -f ingress.yaml`
2. ALB Controller cria ALB na AWS
3. ALB leva 5-10 minutos para ficar "Active"
4. Timeout em CI/CD pipelines configurados com 5min max

**MitigaÃ§Ã£o:**
- âœ… **Retry logic:** CI/CD pipelines com retry automÃ¡tico (3 tentativas)
- âœ… **Timeout extension:** Aumentar timeout para 15 minutos
- âœ… **Health checks:** Validar ALB active antes de prosseguir

**NÃ£o Ã© Bug:**
- Comportamento esperado AWS (ALB provisioning time)
- NÃ£o hÃ¡ mitigaÃ§Ã£o tÃ©cnica (Ã© limitaÃ§Ã£o AWS)

---

### R-012: Cluster Autoscaler Scale-Down Agressivo

**Probabilidade:** BAIXO
**Impacto:** MÃ‰DIO
**Severidade:** ğŸŸ¡ MÃ‰DIO
**Status:** âœ… Mitigado

**DescriÃ§Ã£o:**
Cluster Autoscaler remove nodes prematuramente, causando reschedule desnecessÃ¡rio de pods.

**CenÃ¡rio:**
1. Carga reduz (ex: fim de horÃ¡rio comercial)
2. Cluster Autoscaler detecta nodes com baixa utilizaÃ§Ã£o
3. Scale-down remove node apÃ³s 5 minutos idle
4. Pods rescheduled para outros nodes (I/O spike temporÃ¡rio)
5. PossÃ­vel breve indisponibilidade (30s-1min)

**MitigaÃ§Ãµes:**
- âœ… **Threshold 5 min:** Node precisa estar idle por 5 minutos antes de remoÃ§Ã£o
- âœ… **PodDisruptionBudgets:** Configurados para platform services (prevent simultaneous eviction)
- âœ… **Node taint tolerations:** Critical workloads em nodes dedicated (nÃ£o removidos)

**ConfiguraÃ§Ã£o:**
```yaml
--scale-down-unneeded-time: 5m
--scale-down-delay-after-add: 10m
```

**Monitoramento:**
- Prometheus: Node count histÃ³rico (detectar flapping)
- Alert: Node removed > 2Ã— em 1 hora (indica scale-down agressivo)

---

## ğŸ“ˆ TendÃªncias de Riscos

### Riscos Emergentes (Marco 3)
- **R-013:** GitLab CE single point of failure (sem HA configurado)
- **R-014:** Backup & DR strategy inexistente (loss tolerance: 24h RPO)
- **R-015:** PostgreSQL RDS sem Multi-AZ (custo vs HA trade-off)
- **R-016:** Secrets rotation policy inexistente (compliance risk)

---

## ğŸ”„ Processo de Review

### FrequÃªncia
- **Semanal:** Review de riscos ALTOS
- **Mensal:** Review de todos os riscos + atualizaÃ§Ã£o matriz
- **Ad-hoc:** ApÃ³s incidentes ou mudanÃ§as arquiteturais

### ResponsÃ¡veis
- **DevOps Lead:** Dono da matriz de riscos
- **Security Specialist:** Review de riscos de seguranÃ§a
- **FinOps:** Review de riscos de custo

---

**Mantenedor:** DevOps Team
**Ãšltima RevisÃ£o:** 2026-01-29
**PrÃ³xima RevisÃ£o:** 2026-02-05 (Marco 3 planning)
