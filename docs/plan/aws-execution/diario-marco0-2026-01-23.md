# Di√°rio de Bordo - Marco 0

## 2026-01-26 - Sess√£o 5: Marco 1 COMPLETO - Cluster EKS Provisionado com Sucesso

### üìã Resumo Executivo
- ‚úÖ **MARCO 1 COMPLETO**: Cluster EKS k8s-platform-prod criado e validado
- ‚úÖ **16 recursos Terraform criados com sucesso**
- ‚úÖ **100% Conformidade IaC**: Todos os recursos criados via Terraform
- ‚úÖ **7 nodes operacionais** (2 system + 3 workloads + 2 critical)
- ‚úÖ **4 add-ons instalados e funcionando**
- ‚è±Ô∏è **Tempo total de provisionamento**: ~15 minutos

### üéØ Contexto Inicial
- Marco 0 completo: Backend Terraform funcional, m√≥dulos criados, documenta√ß√£o completa
- Objetivo: Provisionar cluster EKS completo com 3 node groups e add-ons
- Estrat√©gia: CLI-First com 100% conformidade IaC via Terraform
- Decis√£o cr√≠tica: Usu√°rio priorizou conformidade IaC sobre velocidade

### üîß A√ß√µes Realizadas

#### 1. Prepara√ß√£o e Estrutura Terraform (Sess√£o 4)
- ‚úÖ **Tags Kubernetes adicionadas √†s subnets existentes**:
  - Public subnets: `kubernetes.io/role/elb=1`
  - Private subnets: `kubernetes.io/role/internal-elb=1`
  - All subnets: `kubernetes.io/cluster/k8s-platform-prod=shared`

- ‚úÖ **IAM Roles validados** (j√° existentes):
  - Cluster role: `k8s-platform-eks-cluster-role` com AmazonEKSClusterPolicy
  - Node role: `k8s-platform-eks-node-role` com 4 pol√≠ticas necess√°rias

- ‚úÖ **C√≥digo Terraform completo criado**:
  - `platform-provisioning/aws/kubernetes/terraform/envs/marco1/main.tf`
  - `platform-provisioning/aws/kubernetes/terraform/envs/marco1/variables.tf`
  - `platform-provisioning/aws/kubernetes/terraform/envs/marco1/outputs.tf`
  - `platform-provisioning/aws/kubernetes/terraform/envs/marco1/terraform.tfvars`
  - `platform-provisioning/aws/kubernetes/terraform/envs/marco1/backend.tf`

#### 2. Resolu√ß√£o de Problemas de State

**Problema 1: Cluster EKS j√° existia parcialmente**
- Causa: Tentativas anteriores de cria√ß√£o via AWS CLI
- Solu√ß√£o: Tentativa de import para o state do Terraform
- Resultado: Import criou drift e inconsist√™ncias

**Problema 2: M√∫ltiplos locks do DynamoDB**
- Causa: Interrup√ß√µes durante opera√ß√µes do Terraform
- Locks encontrados: 4 diferentes lock IDs
- Solu√ß√£o: `terraform force-unlock -force <LOCK_ID>` para cada lock

**Problema 3: Terraform queria destruir e recriar cluster**
- Causa: State drift ap√≥s tentativa de import
- Op√ß√µes apresentadas:
  - A) AWS CLI (mais r√°pido, menos conformidade IaC)
  - B) Destruir via Terraform e recriar (mais lento, 100% conformidade IaC)
- **Decis√£o do usu√°rio**: OP√á√ÉO B
- Justificativa: "eu prefiro perder esse tempo agora, mas criar com 100% de conformidade com o IaC que estamos montando com o Terraform"

#### 3. Destrui√ß√£o Limpa da Infraestrutura Parcial

```bash
terraform destroy -auto-approve
```

- ‚è±Ô∏è **Tempo de destrui√ß√£o**: 3m47s
- üóëÔ∏è **Recursos destru√≠dos**: 9 recursos
  - aws_eks_cluster.main
  - aws_kms_key.eks
  - aws_kms_alias.eks
  - aws_security_group.eks_cluster
  - aws_security_group.eks_nodes
  - 4 aws_security_group_rule
- ‚úÖ **State limpo** e pronto para rebuild

#### 4. Provisionamento Completo via Terraform

```bash
cd /home/gilvangalindo/projects/Arquitetura/Kubernetes/platform-provisioning/aws/kubernetes/terraform/envs/marco1
export AWS_PROFILE=k8s-platform-prod
terraform apply -auto-approve 2>&1 | tee /tmp/terraform-apply-complete.log
```

**Timeline de Cria√ß√£o:**

**Fase 1: Seguran√ßa e Criptografia (0-15s)**
- ‚úÖ Security Group eks_cluster: 3s (sg-05403c6b017e5ce9a)
- ‚úÖ Security Group eks_nodes: 4s (sg-0a7c2357394844472)
- ‚úÖ 4 Security Group Rules: 1s cada
- ‚úÖ KMS Key: 11s (3e1f7e71-1a23-4de8-88a8-5b01f2606b25)
- ‚úÖ KMS Alias: 0s (alias/k8s-platform-prod-eks-secrets)

**Fase 2: EKS Cluster (0-11m7s)**
- üîÑ Cluster creation: 11m7s
- ‚úÖ Cluster criado: k8s-platform-prod
- ‚úÖ Endpoint: https://9A2B4E51419C283EC7FC49A826EB2E7D.sk1.us-east-1.eks.amazonaws.com
- ‚úÖ Version: 1.31
- ‚úÖ Encryption: KMS habilitado
- ‚úÖ Logs: 5 tipos de logs habilitados (api, audit, authenticator, controllerManager, scheduler)

**Fase 3: Node Groups (11m7s - 13m8s)**
- ‚úÖ Node Group workloads: 1m39s (k8s-platform-prod:workloads)
  - Instance type: t3.large
  - Desired/Min/Max: 3/2/6
  - Labels: node-type=workloads, workload=applications
- ‚úÖ Node Group critical: 2m0s (k8s-platform-prod:critical)
  - Instance type: t3.xlarge
  - Desired/Min/Max: 2/2/4
  - Labels: node-type=critical, workload=databases
  - Taint: workload=critical:NO_SCHEDULE
- ‚úÖ Node Group system: 2m1s (k8s-platform-prod:system)
  - Instance type: t3.medium
  - Desired/Min/Max: 2/2/4
  - Labels: node-type=system, workload=platform

**Fase 4: Add-ons EKS (13m8s - 14m36s)**
- ‚úÖ coredns: 16s (v1.11.3-eksbuild.2)
- ‚úÖ kube-proxy: 47s (v1.31.2-eksbuild.3)
- ‚úÖ ebs-csi-driver: 48s (v1.37.0-eksbuild.1)
- ‚úÖ vpc-cni: 1m28s (v1.18.5-eksbuild.1)

**üìä Resultado Final:**
```
Apply complete! Resources: 16 added, 0 changed, 0 destroyed.
```

#### 5. Valida√ß√£o do Cluster

**Configura√ß√£o kubectl:**
```bash
aws eks update-kubeconfig --region us-east-1 --name k8s-platform-prod --profile k8s-platform-prod
```
‚úÖ Contexto adicionado: `arn:aws:eks:us-east-1:891377105802:cluster/k8s-platform-prod`

**Valida√ß√£o de Nodes:**
```bash
kubectl get nodes -L node-type,workload,eks.amazonaws.com/nodegroup
```

| Node | Status | Node-Type | Workload | Node Group | Instance Type |
|------|--------|-----------|----------|------------|---------------|
| ip-10-0-128-205 | Ready | critical | databases | critical | t3.xlarge |
| ip-10-0-129-26 | Ready | workloads | applications | workloads | t3.large |
| ip-10-0-135-121 | Ready | workloads | applications | workloads | t3.large |
| ip-10-0-139-209 | Ready | system | platform | system | t3.medium |
| ip-10-0-147-141 | Ready | workloads | applications | workloads | t3.large |
| ip-10-0-151-187 | Ready | system | platform | system | t3.medium |
| ip-10-0-155-78 | Ready | critical | databases | critical | t3.xlarge |

**Valida√ß√£o de Pods do Sistema:**
```bash
kubectl get pods -n kube-system
```

‚úÖ **Todos os pods em estado Running:**
- CoreDNS: 2 pods Running
- VPC CNI (aws-node): 7 pods Running (1 por node)
- Kube-proxy: 7 pods Running (1 por node)
- EBS CSI Controller: 2 pods Running
- EBS CSI Node: 7 pods Running (1 por node)

### üìà M√©tricas de Sucesso

| M√©trica | Valor | Status |
|---------|-------|--------|
| Recursos Terraform | 16 | ‚úÖ 100% |
| Nodes provisionados | 7 | ‚úÖ 100% |
| Nodes Ready | 7/7 | ‚úÖ 100% |
| Add-ons instalados | 4/4 | ‚úÖ 100% |
| Pods sistema Running | 25/25 | ‚úÖ 100% |
| Conformidade IaC | 100% | ‚úÖ Objetivo alcan√ßado |
| Tempo total | ~15min | ‚úÖ Dentro do esperado |

### üéì Li√ß√µes Aprendidas

1. **Priorizar conformidade IaC desde o in√≠cio**
   - Tentativas de criar recursos via AWS CLI causaram problemas de state
   - Reconstruir via Terraform garantiu documenta√ß√£o completa e rastreabilidade

2. **State management √© cr√≠tico**
   - M√∫ltiplos locks indicam necessidade de melhor controle de processos
   - Import de recursos deve ser evitado quando poss√≠vel
   - Destrui√ß√£o limpa + recria√ß√£o √© prefer√≠vel a tentar corrigir drift

3. **Transpar√™ncia durante provisionamento**
   - Updates frequentes (a cada 30-90s) mant√™m usu√°rio informado
   - Provisionamento de EKS leva ~11 minutos (esperado)
   - Node groups s√£o r√°pidos (~2 minutos) mas nodes levam mais tempo para ficar Ready

4. **Valida√ß√£o completa √© essencial**
   - N√£o basta criar recursos, √© preciso validar pods, nodes, add-ons
   - Labels e taints devem ser verificados
   - Cluster info deve ser documentado para troubleshooting futuro

### üìÅ Artefatos Criados

1. **C√≥digo Terraform**:
   - `platform-provisioning/aws/kubernetes/terraform/envs/marco1/main.tf` (370 linhas)
   - `platform-provisioning/aws/kubernetes/terraform/envs/marco1/variables.tf` (55 linhas)
   - `platform-provisioning/aws/kubernetes/terraform/envs/marco1/outputs.tf` (98 linhas)
   - `platform-provisioning/aws/kubernetes/terraform/envs/marco1/terraform.tfvars` (29 linhas)
   - `platform-provisioning/aws/kubernetes/terraform/envs/marco1/backend.tf` (11 linhas)

2. **Logs de Execu√ß√£o**:
   - `/tmp/terraform-destroy.log` (log da destrui√ß√£o limpa)
   - `/tmp/terraform-apply-complete.log` (log completo do apply)

3. **Configura√ß√£o kubectl**:
   - Context adicionado em `~/.kube/config`

### üéØ Estado Atual

- ‚úÖ **Cluster EKS**: k8s-platform-prod ATIVO
- ‚úÖ **Nodes**: 7 nodes Ready (2 system, 3 workloads, 2 critical)
- ‚úÖ **Add-ons**: 4 add-ons instalados e funcionando
- ‚úÖ **Networking**: VPC CNI configurado, CoreDNS operacional
- ‚úÖ **Storage**: EBS CSI Driver pronto para PVCs
- ‚úÖ **Security**: KMS encryption habilitado, Security Groups configurados
- ‚úÖ **State**: Terraform state limpo e sincronizado com infraestrutura real

### üí∞ Gerenciamento de Custos

**Problema identificado:** Cluster EKS gera custos significativos 24/7 (~$625/m√™s)

**Solu√ß√£o implementada:** Scripts de gest√£o de custos para ligar/desligar cluster

#### Scripts Criados

1. **`status-cluster.sh`** - Verifica status e custos
   - Mostra estado do cluster (ACTIVE/DESLIGADO)
   - Lista node groups e inst√¢ncias
   - Calcula custos por hora/dia/m√™s
   - Valida kubectl e conectividade

2. **`shutdown-cluster.sh`** - Desliga cluster
   - Destr√≥i cluster EKS, nodes, add-ons, security groups, KMS
   - Mant√©m VPC, subnets, NAT gateways, IAM roles
   - Cria backup autom√°tico do Terraform state
   - Tempo: ~3-5 minutos
   - Economia: ~$0.76/hora (~$547/m√™s)

3. **`startup-cluster.sh`** - Liga cluster
   - Recria toda infraestrutura via Terraform (100% IaC)
   - Configura kubectl automaticamente
   - Valida nodes e pods
   - Tempo: ~15 minutos

#### Custos Detalhados

**Com cluster LIGADO:**
- Cluster EKS: $0.10/hora ($73/m√™s)
- 7 Nodes EC2: $0.66/hora ($475/m√™s)
- 2 NAT Gateways: $0.09/hora ($66/m√™s)
- **Total: $0.86/hora (~$625/m√™s)**

**Com cluster DESLIGADO:**
- 2 NAT Gateways: $0.09/hora ($66/m√™s)
- **Total: $0.09/hora (~$66/m√™s)**
- **Economia: $0.76/hora (~$547/m√™s)**

#### Estrat√©gia Recomendada

**Desenvolvimento di√°rio (segunda a sexta):**
```bash
# Manh√£: ligar cluster
./startup-cluster.sh  # ~15 minutos

# Trabalho durante o dia (~10 horas)

# Noite: desligar cluster
./shutdown-cluster.sh  # ~5 minutos
```

**Economia mensal:** ~50% (~$300/m√™s)
- Ligado: 10h/dia √ó 5 dias = 50h/semana = 220h/m√™s
- Custo: 220h √ó $0.86 = ~$189/m√™s + $66 (NAT) = $255/m√™s
- vs. 24/7: $625/m√™s

#### Localiza√ß√£o dos Scripts

```
platform-provisioning/aws/kubernetes/terraform/envs/marco1/scripts/
‚îú‚îÄ‚îÄ status-cluster.sh      # Verificar status e custos
‚îú‚îÄ‚îÄ shutdown-cluster.sh    # Desligar cluster
‚îú‚îÄ‚îÄ startup-cluster.sh     # Ligar cluster
‚îî‚îÄ‚îÄ README.md             # Documenta√ß√£o completa
```

#### Documenta√ß√£o

Documenta√ß√£o completa em:
- [scripts/README.md](../../../platform-provisioning/aws/kubernetes/terraform/envs/marco1/scripts/README.md)

Inclui:
- Guia de uso de cada script
- Tabelas de custos detalhadas
- Estrat√©gias de economia
- Troubleshooting comum
- Conformidade IaC

### üöÄ Pr√≥ximos Passos (Marco 2)

1. Instalar Ingress Controller (AWS Load Balancer Controller)
2. Configurar Cert-Manager para certificados TLS
3. Implementar monitoramento (Prometheus + Grafana)
4. Configurar logging centralizado (Fluent Bit + CloudWatch)
5. Implementar pol√≠ticas de rede (Network Policies)
6. Configurar Auto Scaling (Cluster Autoscaler ou Karpenter)
7. Deploy de aplica√ß√µes de teste

### üí° Observa√ß√µes T√©cnicas

- **VPC**: Utilizando VPC existente `fictor-vpc` (10.0.0.0/16)
- **Subnets**: 2 AZs (us-east-1a, us-east-1b) com 2 private + 2 public subnets
- **Kubernetes Version**: 1.31 (vers√£o mais recente suportada)
- **Container Runtime**: containerd 2.1.5
- **OS**: Amazon Linux 2023.10.20260105
- **Kernel**: 6.1.159-181.297.amzn2023.x86_64

### üîê Recursos de Seguran√ßa

- ‚úÖ KMS encryption para secrets do EKS
- ‚úÖ Security Groups isolando cluster e nodes
- ‚úÖ Private subnets para nodes
- ‚úÖ Public endpoint com restri√ß√£o de CIDR (VPC CIDR only)
- ‚úÖ IAM roles com pol√≠ticas espec√≠ficas (least privilege)
- ‚úÖ Logs de auditoria habilitados (5 tipos)

---

## 2026-01-26 - Sess√£o 4: Prepara√ß√£o para Marco 1 - Provisionamento EKS Cluster

- Contexto inicial:
  - Marco 0 COMPLETO: Backend Terraform funcional, m√≥dulos criados, documenta√ß√£o completa
  - Objetivo: Avan√ßar para Marco 1 (Provisionamento EKS Cluster)
  - Estrat√©gia: CLI-First (Terraform/AWS CLI) com documenta√ß√£o cont√≠nua no di√°rio

- Verifica√ß√µes de ambiente:
  - ‚úÖ Terraform instalado: v1.14.3
  - ‚úÖ kubectl instalado: v1.34.1
  - ‚ö†Ô∏è **Credenciais AWS expiradas**: Necess√°rio renovar via `aws login`
  - ‚úÖ Diret√≥rio de trabalho: `/home/gilvangalindo/projects/Arquitetura/Kubernetes`

- A√ß√µes realizadas:
  - ‚úÖ **Credenciais AWS validadas com sucesso**:
    - Profile: `k8s-platform-prod`
    - Account: `891377105802`
    - User: `gilvan.galindo`
    - Role: `AWSReservedSSO_AdministratorAccess`

  - ‚úÖ **An√°lise da infraestrutura AWS atual**:
    - **Clusters EKS**: Nenhum cluster EKS existente
    - **VPC existente**: `vpc-0b1396a59c417c1f0` (10.0.0.0/16) - Nome: `fictor-vpc`
    - **Subnets existentes**:
      - `subnet-0b5e0cae5658ea993` (10.0.0.0/20) - public1-us-east-1a
      - `subnet-07dca8ceb9882ba66` (10.0.16.0/20) - public2-us-east-1b
      - `subnet-0472ab28726cdf745` (10.0.128.0/20) - private1-us-east-1a
      - `subnet-0288a67cd352effa7` (10.0.144.0/20) - private2-us-east-1b

- Situa√ß√£o identificada:
  - VPC j√° existe (reverse-engineered no Marco 0)
  - Nenhum cluster EKS criado ainda
  - Infraestrutura de rede b√°sica pronta (2 AZs com subnets p√∫blicas e privadas)

- Decis√£o estrat√©gica necess√°ria:
  **OP√á√ÉO A**: Criar cluster EKS na VPC existente (`fictor-vpc`)
  - Vantagens: Usa infraestrutura existente, alinhado com Marco 0
  - Pr√≥ximos passos: Criar EKS cluster + Node Groups via Terraform

  **OP√á√ÉO B**: Criar nova VPC dedicada para plataforma Kubernetes
  - Vantagens: Isolamento completo, configura√ß√£o ideal desde o in√≠cio
  - Pr√≥ximos passos: Provisionar nova VPC + EKS cluster

- **DECIS√ÉO TOMADA**: ‚úÖ OP√á√ÉO A - Usar VPC existente (`fictor-vpc`)
  - Justificativa: Alinhado com Marco 0, infraestrutura j√° validada, economia de recursos
  - Estrat√©gia incremental: Iniciar com 2 AZs, criar script para adicionar 3¬™ AZ quando necess√°rio
  - Abordagem: Tags Kubernetes + EKS Cluster + 3 Node Groups

- An√°lise de recursos adicionais necess√°rios:
  - Verificando NAT Gateways, Internet Gateways, Route Tables
  - Identificando necessidade de tags Kubernetes nas subnets
  - Validando IAM roles necess√°rias

- Pr√≥ximas a√ß√µes imediatas:
  1. Analisar recursos de rede existentes (NAT, IGW, Route Tables)
  2. Adicionar tags Kubernetes nas subnets existentes
  3. Criar IAM roles para EKS cluster e node groups
  4. Preparar c√≥digo Terraform para EKS cluster (2 AZs inicialmente)
  5. Criar script incremental para adicionar 3¬™ AZ (us-east-1c)
  6. Executar `terraform plan` para review
  7. Ap√≥s aprova√ß√£o, executar `terraform apply`
  8. Validar cluster EKS criado
  9. Documentar todos os passos

---

## 2026-01-24 - Sess√£o 3: Ajuste de Scripts e Documenta√ß√£o Completa

- A√ß√µes realizadas:
  - **Corre√ß√£o do script create-tf-backend.sh**:
    - ‚ùå **BUG ENCONTRADO**: Script original falhava em us-east-1 com `InvalidLocationConstraint`
    - ‚úÖ **FIX APLICADO**: Adicionada verifica√ß√£o para us-east-1 (n√£o usa LocationConstraint)
    - ‚úÖ Melhorado feedback com mensagens de recurso j√° existente
    - ‚úÖ Adicionado `aws dynamodb wait table-exists` para garantir tabela ativa
  - **Criados scripts auxiliares para marco0**:
    - ‚úÖ `init-terraform.sh`: Carrega credenciais AWS automaticamente e executa terraform init
    - ‚úÖ `plan-terraform.sh`: Carrega credenciais e executa terraform plan
    - ‚úÖ Ambos scripts suportam credenciais do cache AWS CLI (SSO/login)
  - **Documenta√ß√£o completa criada**:
    - ‚úÖ `COMANDOS-EXECUTADOS-MARCO0.md`: Documento detalhado com TODOS os comandos AWS CLI
    - ‚úÖ Explica√ß√µes t√©cnicas de cada par√¢metro
    - ‚úÖ Diagrams de funcionamento do backend S3/DynamoDB
    - ‚úÖ Troubleshooting comum e solu√ß√µes
    - ‚úÖ An√°lise de custos ($0.01/m√™s estimado)

- Problemas encontrados e solu√ß√µes:
  1. **Problema**: InvalidLocationConstraint ao criar bucket em us-east-1
     - **Causa**: us-east-1 √© regi√£o especial, n√£o aceita LocationConstraint
     - **Solu√ß√£o**: Condicional no script para detectar us-east-1
     - **Aprendizado**: Outras regi√µes REQUEREM LocationConstraint

  2. **Problema**: Terraform init falhando com "No valid credential sources found"
     - **Causa**: Terraform backend n√£o conseguia acessar credenciais do AWS CLI
     - **Solu√ß√£o**: Exportar AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN
     - **Aprendizado**: Credenciais STS (ASIA...) requerem SESSION_TOKEN obrigat√≥rio

  3. **Problema**: State lock persistente ap√≥s Ctrl+C
     - **Causa**: Terraform n√£o conseguiu executar cleanup (DeleteItem no DynamoDB)
     - **Solu√ß√£o**: `terraform force-unlock <LOCK_ID>`
     - **Aprendizado**: Sempre verificar se h√° processos rodando antes de force-unlock

  4. **Problema**: terraform plan mostra "will create" para recursos existentes
     - **Causa**: Recursos existentes n√£o foram importados para o state
     - **Solu√ß√£o**: DECIS√ÉO ARQUITETURAL - n√£o importar, usar c√≥digo como blueprint
     - **Aprendizado**: Import √© tedioso (1 comando por recurso), c√≥digo serve melhor como template

- Estado atual:
  - Scripts corrigidos e testados
  - Documenta√ß√£o t√©cnica completa (20+ p√°ginas)
  - Backend funcional e validado
  - Credenciais carregadas automaticamente via scripts

- Pr√≥ximas a√ß√µes:
  - Commitar scripts e documenta√ß√£o
  - Atualizar README principal com link para COMANDOS-EXECUTADOS-MARCO0.md
  - Marco 0 considerado COMPLETO

---

## 2026-01-24 - Sess√£o 2: Execu√ß√£o Completa Marco 0 (Backend + Valida√ß√µes)

- A√ß√µes realizadas (sess√£o 2):
  - **Bootstrap do Backend Terraform executado com sucesso**:
    - Bucket S3 criado: `terraform-state-marco0-891377105802`
    - Versionamento habilitado
    - Criptografia AES256 configurada
    - Public access bloqueado
    - Tabela DynamoDB criada: `terraform-state-lock`
    - Billing mode: PAY_PER_REQUEST
  - **Backend.tf configurado** com valores do bucket e tabela
  - **terraform.tfvars criado** com valores reais da infraestrutura
  - **Terraform init executado com sucesso** com backend remoto S3
  - **State file criado** no S3 (marco0/terraform.tfstate)
  - **Lock mechanism testado** via DynamoDB (force-unlock executado)

- Observa√ß√µes t√©cnicas importantes:
  - Terraform plan mostra cria√ß√£o de recursos (expected) porque os recursos existentes N√ÉO foram importados para o state
  - Para obter "No changes" seria necess√°rio executar `terraform import` para cada recurso:

    ```bash
    terraform import module.vpc.aws_vpc.vpc vpc-0b1396a59c417c1f0
    terraform import module.subnets.aws_subnet.subnets["subnet-xyz"] subnet-xyz
    # ... para cada recurso
    ```

  - **Decis√£o arquitetural**: Manter c√≥digo como "blueprint" para novas regi√µes/ambientes ao inv√©s de importar infraestrutura existente
  - C√≥digo validado localmente (terraform validate) e estrutura est√° correta

- Estado atual:
  - Backend Terraform funcional (S3 + DynamoDB)
  - C√≥digo Terraform modular e reutiliz√°vel
  - State file versionado e criptografado
  - Pronto para criar novas infraestruturas (novos ambientes, regi√µes)

- Pr√≥ximas a√ß√µes (opcional):
  1. Se necess√°rio gerenciar infra existente via Terraform: executar imports
  2. OU usar o c√≥digo como template para novos ambientes (marco1, marco2, etc.)
  3. Adicionar EKS cluster provisioning aos m√≥dulos
  4. Criar ambientes adicionais (staging, production)

---

## 2026-01-24 - Commit e Consolida√ß√£o Marco 0

- A√ß√µes realizadas:
  - Executado `00-marco0-reverse-engineer-vpc.sh` em CloudShell (usu√°rio), gerando JSONs: vpc.json, subnets.json, nat-gateways.json, route-tables.json, internet-gateway.json, security-groups.json
  - Processados JSONs e gerados m√≥dulos Terraform: vpc, subnets, nat-gateways, route-tables, internet-gateway, security-groups, kms
  - Copiados m√≥dulos para `platform-provisioning/aws/kubernetes/terraform/modules/`
  - Criado ambiente marco0 em `platform-provisioning/aws/kubernetes/terraform/envs/marco0/` com main.tf, backend.tf, variables.tf, outputs.tf, terraform.tfvars.example
  - Corrigidos erros de sintaxe: removidas vari√°veis duplicadas, corrigidos outputs do m√≥dulo subnets (filtragem public/private)
  - Valida√ß√£o local: `terraform init -backend=false` (sucesso), `terraform validate` (sucesso)
  - **Consolidada documenta√ß√£o no README.md principal** com se√ß√£o dedicada ao Marco 0
  - **Criados ponteiros README.MD.INFRA** em todos os diret√≥rios seguindo governan√ßa documental
  - **Removidos READMEs duplicados** para atender hook de valida√ß√£o de governan√ßa
  - **Commit criado com sucesso**: `420b043` - "feat: add Marco 0 VPC reverse engineering and Terraform infrastructure"
    - 40 arquivos alterados, 2156 inser√ß√µes, 185 dele√ß√µes
    - Hook de valida√ß√£o documental passou com sucesso

- Estado atual:
  - Configura√ß√£o Terraform v√°lida e equivalente √† infraestrutura existente (VPC 10.0.0.0/16, 4 subnets, 2 NATs, IGW, route tables)
  - Backend S3 configurado parcialmente (aguardando bootstrap com credenciais)
  - **C√≥digo versionado e documentado** seguindo padr√µes de governan√ßa do projeto
  - **Estrutura modular completa** pronta para reutiliza√ß√£o em outros ambientes
  - Pronto para: bootstrap backend, terraform plan com credenciais, valida√ß√µes de equival√™ncia

- Pr√≥ximas a√ß√µes t√©cnicas:
  1. Executar `create-tf-backend.sh` com credenciais para criar S3 bucket e DynamoDB table
  2. Completar `backend.tf` e executar `terraform init` com backend remoto
  3. Executar `terraform plan` em CloudShell para confirmar "No changes" (equival√™ncia)
  4. Implementar adi√ß√µes incrementais: subnets EKS (10.0.40-55.0/24) via atualiza√ß√£o main.tf
  5. Executar valida√ß√µes: isolamento rede, tags K8s, conectividade NAT, smoke tests

- Observa√ß√µes:
  - Configura√ß√£o validada localmente e commitada
  - Governan√ßa documental respeitada (README √∫nico na raiz + ponteiros README.MD.INFRA)
  - Pr√≥ximos passos requerem credenciais AWS para execu√ß√£o em CloudShell

---

## 2026-01-23 - Execu√ß√£o Marco 0 (registro inicial)

- Contexto recuperado de `docs/plan/aws-console-execution-plan.md` e demais arquivos em `docs/plan/aws-execution/`.

- Pre-hook (inten√ß√£o):
  - Tipo: feature
  - Dom√≠nio afetado: `platform-provisioning/aws` (infraestrutura)
  - Artefatos afetados: IaC, scripts, documenta√ß√£o
  - Risco estimado: m√©dio
  - Necessita ADR?: n√£o
  - Afeta outros dom√≠nios?: n√£o (valida√ß√µes via contratos/documenta√ß√£o)

- A√ß√µes iniciadas (artefatos criados):
  - `docs/plan/aws-execution/scripts/00-marco0-reverse-engineer-vpc.sh` (esbo√ßo, modo dry-run)
  - `docs/plan/aws-execution/scripts/01-marco0-incremental-add-region.sh` (esbo√ßo, dry-run)
  - `platform-provisioning/aws/kubernetes/terraform-backend/create-tf-backend.sh` (script bootstrap S3 + DynamoDB)
  - Estrutura inicial Terraform: `platform-provisioning/aws/kubernetes/terraform/` com `modules/` e `envs/marco0/` placeholders

- Pr√≥ximas a√ß√µes t√©cnicas:
  1. Executar `00-marco0-reverse-engineer-vpc.sh` em modo dry-run e coletar outputs JSON.
  2. Gerar c√≥digo Terraform na pasta `vpc-reverse-engineered/terraform` e executar `terraform plan` para validar equival√™ncia com o estado atual.
  3. Executar `create-tf-backend.sh` em ambiente controlado para criar bucket S3 e DynamoDB lock (bootstrap do backend remoto).
  4. Preencher `envs/marco0/backend.tf` com valores do backend e iniciar `terraform init`.
  5. Planejar e executar valida√ß√µes: isolamento de rede (EC2 test), tags Kubernetes nas subnets, conectividade NAT, smoke tests de cria√ß√£o/dele√ß√£o.

- Observa√ß√µes de governan√ßa: seguir o prompt `docs/prompts/develop-feature.md` (pr√©-hook, execu√ß√£o ordenada e post-hook). Registrar commits conforme padr√£o do projeto.

---

Arquivo gerado automaticamente em: 2026-01-23
Autor: DevOps Team

---

## 2026-01-26 - Sess√£o 6: Marco 2 - Platform Services (AWS Load Balancer Controller)

### üìã Resumo Executivo
- ‚úÖ **MARCO 2 - FASE 1 COMPLETO**: AWS Load Balancer Controller instalado e validado
- ‚úÖ **6 recursos criados com sucesso** (OIDC Provider, IAM Policy/Role, Service Account, Helm Release)
- ‚úÖ **100% Conformidade IaC**: Todos os recursos criados via Terraform
- ‚úÖ **Ingress Controller funcional**: ALB criado automaticamente, targets healthy, HTTP 200 OK
- ‚è±Ô∏è **Tempo total de instala√ß√£o**: ~3 minutos (OIDC + IAM) + ~40 segundos (Helm)

### üéØ Contexto Inicial
- Marco 1 completo: Cluster EKS com 7 nodes operacionais
- Objetivo: Instalar AWS Load Balancer Controller para habilitar Ingress/ALB
- Necessidade: OIDC Provider n√£o existia (pr√©-requisito para IRSA)
- Estrat√©gia: Terraform modular + Helm para instala√ß√£o cloud-agnostic

### üîß A√ß√µes Realizadas

#### 1. Estrutura Marco 2 Criada
**Diret√≥rios:**
```
platform-provisioning/aws/kubernetes/terraform/envs/marco2/
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îî‚îÄ‚îÄ aws-load-balancer-controller/
‚îÇ       ‚îú‚îÄ‚îÄ main.tf              # IRSA + Helm chart
‚îÇ       ‚îú‚îÄ‚îÄ variables.tf         # Vari√°veis do m√≥dulo
‚îÇ       ‚îú‚îÄ‚îÄ outputs.tf           # ARNs e nomes
‚îÇ       ‚îú‚îÄ‚îÄ versions.tf          # Provider requirements
‚îÇ       ‚îî‚îÄ‚îÄ iam-policy.json      # Policy oficial AWS v2.11.0
‚îú‚îÄ‚îÄ main.tf                      # OIDC Provider + m√≥dulo ALB
‚îú‚îÄ‚îÄ providers.tf                 # AWS + Kubernetes + Helm + TLS providers
‚îú‚îÄ‚îÄ backend.tf                   # S3 state (marco2/terraform.tfstate)
‚îú‚îÄ‚îÄ variables.tf                 # VPC ID, cluster name, region
‚îú‚îÄ‚îÄ outputs.tf                   # Outputs do Marco 2
‚îú‚îÄ‚îÄ terraform.tfvars             # Valores do ambiente
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ init-terraform.sh        # Inicializa√ß√£o com credenciais
    ‚îú‚îÄ‚îÄ plan-terraform.sh        # Terraform plan
    ‚îî‚îÄ‚îÄ apply-terraform.sh       # Apply com confirma√ß√£o
```

#### 2. OIDC Provider para EKS
**Problema identificado:**
- Data source tentava buscar OIDC provider inexistente
- Erro: `finding IAM OIDC Provider by url (...): not found`

**Solu√ß√£o implementada:**
- Cria√ß√£o do OIDC Provider via Terraform no `main.tf`
- Uso do provider `hashicorp/tls` para obter thumbprint do certificado
- Provider configurado com:
  - URL: `https://oidc.eks.us-east-1.amazonaws.com/id/5C0C8E8002CF20AB8918B1752442BF79`
  - Client ID: `sts.amazonaws.com`
  - Thumbprint: `06b25927c42a721631c1efd9431e648fa62e1e39`

**Resultado:**
```
aws_iam_openid_connect_provider.eks: Created
ARN: arn:aws:iam::891377105802:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/5C0C8E8002CF20AB8918B1752442BF79
```

#### 3. AWS Load Balancer Controller - M√≥dulo Terraform
**Recursos criados pelo m√≥dulo:**

1. **IAM Policy** - Permiss√µes para gerenciar ALB/NLB
   - Nome: `AWSLoadBalancerControllerIAMPolicy-k8s-platform-prod`
   - ARN: `arn:aws:iam::891377105802:policy/AWSLoadBalancerControllerIAMPolicy-k8s-platform-prod`
   - Source: [AWS oficial v2.11.0](https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.11.0/docs/install/iam_policy.json)

2. **IAM Role** - IRSA (IAM Roles for Service Accounts)
   - Nome: `AWSLoadBalancerControllerRole-k8s-platform-prod`
   - ARN: `arn:aws:iam::891377105802:role/AWSLoadBalancerControllerRole-k8s-platform-prod`
   - Trust policy: Service Account `kube-system/aws-load-balancer-controller`

3. **Kubernetes Service Account**
   - Nome: `aws-load-balancer-controller`
   - Namespace: `kube-system`
   - Annotation: `eks.amazonaws.com/role-arn` com ARN da IAM Role

4. **Helm Release** - AWS Load Balancer Controller
   - Chart: `aws-load-balancer-controller` v1.11.0
   - Repository: `https://aws.github.io/eks-charts`
   - Namespace: `kube-system`
   - Replicas: 2 (default)
   - Node Selector: `node-type=system`
   - Tolerations: `node-type=system:NoSchedule`

**Configura√ß√µes do Helm:**
- `clusterName`: k8s-platform-prod
- `region`: us-east-1
- `vpcId`: vpc-0b1396a59c417c1f0
- `serviceAccount.create`: false (usamos SA criada pelo Terraform)
- Features desabilitadas (custo): Shield, WAF, WAFv2

#### 4. Valida√ß√£o Completa

**a) Status do Deployment:**
```bash
$ kubectl get deployment -n kube-system aws-load-balancer-controller
NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
aws-load-balancer-controller   2/2     2            2           25s
```

**b) Pods Running:**
```bash
$ kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
NAME                                            READY   STATUS    RESTARTS   AGE
aws-load-balancer-controller-67555dfd56-5vmxw   1/1     Running   0          26s
aws-load-balancer-controller-67555dfd56-sf5rc   1/1     Running   0          26s
```

**c) Teste com Ingress:**
Criado namespace `test-alb` com:
- Deployment nginx (2 replicas) em nodes workloads
- Service ClusterIP na porta 80
- Ingress com annotations para ALB internet-facing

**Recursos AWS criados automaticamente pelo controller:**
```
‚úÖ Security Group: k8s-testalb-nginxtes-16dfe0f4c5
‚úÖ Target Group: k8s-testalb-nginxtes-e62941bc69
   - ARN: arn:aws:elasticloadbalancing:us-east-1:891377105802:targetgroup/k8s-testalb-nginxtes-e62941bc69/49185039e4473ba8
   - Targets: 2/2 healthy (10.0.132.244:80, 10.0.157.147:80)
‚úÖ Application Load Balancer: k8s-testalb-nginxtes-ce8b024b2a
   - ARN: arn:aws:elasticloadbalancing:us-east-1:891377105802:loadbalancer/app/k8s-testalb-nginxtes-ce8b024b2a/0ee3d2e0e231dd18
   - DNS: k8s-testalb-nginxtes-ce8b024b2a-340076399.us-east-1.elb.amazonaws.com
   - State: active (ap√≥s ~20 segundos de provisioning)
   - Subnets: public1-us-east-1a, public2-us-east-1b
‚úÖ Listener: porta 80 HTTP
‚úÖ Listener Rule: rota /* ‚Üí target group
‚úÖ Target Group Binding: Service nginx-test:80
```

**d) Teste HTTP:**
```bash
$ curl -v http://k8s-testalb-nginxtes-ce8b024b2a-340076399.us-east-1.elb.amazonaws.com/
* Connected to (...) (44.196.19.124) port 80
< HTTP/1.1 200 OK
< Server: nginx/1.27.5
< Content-Type: text/html
< Content-Length: 615

<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...
```

‚úÖ **Resultado:** HTTP 200 OK, nginx respondendo corretamente atrav√©s do ALB

**e) Logs do Controller:**
```json
{"level":"info","msg":"successfully built model","model":"test-alb/nginx-test"}
{"level":"info","msg":"creating targetGroup","stackID":"test-alb/nginx-test"}
{"level":"info","msg":"created targetGroup","arn":"..."}
{"level":"info","msg":"creating loadBalancer","stackID":"test-alb/nginx-test"}
{"level":"info","msg":"created loadBalancer","arn":"..."}
{"level":"info","msg":"creating listener","stackID":"test-alb/nginx-test"}
{"level":"info","msg":"created listener","arn":"..."}
{"level":"info","msg":"creating listener rule"}
{"level":"info","msg":"created listener rule"}
{"level":"info","msg":"successfully deployed model","ingressGroup":"test-alb/nginx-test"}
```

#### 5. Limpeza de Recursos de Teste
```bash
$ kubectl delete namespace test-alb
namespace "test-alb" deleted
```
‚úÖ ALB e recursos AWS removidos automaticamente pelo controller (cleanup completo)

### üìä Recursos Terraform Criados (Marco 2)

| Recurso | Nome | ARN/ID | Status |
|---------|------|--------|--------|
| OIDC Provider | eks-oidc-provider-k8s-platform-prod | arn:aws:iam::891377105802:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/5C0C8E8002CF20AB8918B1752442BF79 | ‚úÖ Created |
| IAM Policy | AWSLoadBalancerControllerIAMPolicy-k8s-platform-prod | arn:aws:iam::891377105802:policy/AWSLoadBalancerControllerIAMPolicy-k8s-platform-prod | ‚úÖ Created |
| IAM Role | AWSLoadBalancerControllerRole-k8s-platform-prod | arn:aws:iam::891377105802:role/AWSLoadBalancerControllerRole-k8s-platform-prod | ‚úÖ Created |
| IAM Role Policy Attachment | - | AWSLoadBalancerControllerRole-k8s-platform-prod-20260126170417502600000001 | ‚úÖ Created |
| K8s Service Account | aws-load-balancer-controller | kube-system/aws-load-balancer-controller | ‚úÖ Created |
| Helm Release | aws-load-balancer-controller | aws-load-balancer-controller (v1.11.0) | ‚úÖ Created |

**Total:** 6 recursos

### üí∞ Impacto em Custos

**Recursos permanentes (sem custo):**
- OIDC Provider: gratuito
- IAM Policy/Role: gratuito
- Service Account: gratuito
- Pods do controller: rodando em nodes existentes (sem custo adicional)

**Recursos sob demanda (pagos quando criados):**
- Application Load Balancer: ~$0.0225/hora (~$16.20/m√™s) quando Ingress √© criado
- Target Groups: inclu√≠do no custo do ALB
- Security Groups: gratuito

**Observa√ß√£o importante:**
- ALBs s√£o criados APENAS quando um Ingress √© provisionado
- Quando o Ingress √© deletado, o ALB √© removido automaticamente
- **Nenhum custo adicional permanente**, apenas custos sob demanda por aplica√ß√£o

### üéØ Decis√µes Arquiteturais

1. **OIDC Provider criado via Terraform:**
   - Rationale: Necess√°rio para IRSA (IAM Roles for Service Accounts)
   - Benef√≠cio: Permite que pods assumam IAM roles sem AWS credentials est√°ticas
   - Seguran√ßa: Least privilege, rota√ß√£o autom√°tica de tokens

2. **M√≥dulo reutiliz√°vel para ALB Controller:**
   - Localiza√ß√£o: `envs/marco2/modules/aws-load-balancer-controller/`
   - Benef√≠cio: Pode ser reutilizado em outros ambientes (staging, dev)
   - Versionamento: Chart version parametrizado (1.11.0)

3. **Node Selector + Tolerations para system nodes:**
   - Controller roda APENAS em nodes do tipo `system`
   - Evita usar nodes `workloads` ou `critical`
   - Alinhado com strategy de Marco 1

4. **Backend state separado:**
   - State path: `marco2/terraform.tfstate`
   - Benef√≠cio: Isolamento entre Marcos
   - Permite rollback independente de cada Marco

5. **Features AWS desabilitadas por padr√£o:**
   - Shield, WAF, WAFv2 = false
   - Rationale: Economia de custos em ambiente de desenvolvimento
   - Possibilidade de habilitar em produ√ß√£o via vari√°vel

### üìù Arquivos Importantes

**Terraform:**
- `platform-provisioning/aws/kubernetes/terraform/envs/marco2/main.tf`
- `platform-provisioning/aws/kubernetes/terraform/envs/marco2/modules/aws-load-balancer-controller/main.tf`
- `platform-provisioning/aws/kubernetes/terraform/envs/marco2/modules/aws-load-balancer-controller/iam-policy.json`

**Scripts:**
- `platform-provisioning/aws/kubernetes/terraform/envs/marco2/scripts/init-terraform.sh`
- `platform-provisioning/aws/kubernetes/terraform/envs/marco2/scripts/plan-terraform.sh`
- `platform-provisioning/aws/kubernetes/terraform/envs/marco2/scripts/apply-terraform.sh`

**Testes:**
- `platform-provisioning/aws/kubernetes/terraform/envs/marco2/test-ingress/test-app.yaml`

**Logs:**
- `/tmp/terraform-marco2-apply-20260126_140404.log`

### ‚úÖ Valida√ß√µes Executadas

- ‚úÖ Terraform init com 4 providers (AWS, Kubernetes, Helm, TLS)
- ‚úÖ Terraform plan mostrando 6 recursos a criar
- ‚úÖ Terraform apply bem-sucedido (~3 minutos)
- ‚úÖ OIDC Provider criado e validado via AWS CLI
- ‚úÖ IAM Policy/Role criados com permiss√µes corretas
- ‚úÖ Service Account criada com annotation IRSA
- ‚úÖ Helm chart instalado (v1.11.0)
- ‚úÖ 2 pods do controller Running
- ‚úÖ Deployment 2/2 Ready
- ‚úÖ Ingress de teste criado com sucesso
- ‚úÖ ALB provisionado automaticamente
- ‚úÖ Target Group com 2 targets healthy
- ‚úÖ HTTP 200 OK atrav√©s do ALB
- ‚úÖ Cleanup autom√°tico ao deletar namespace

### üéì Aprendizados e Observa√ß√µes

1. **OIDC Provider √© pr√©-requisito cr√≠tico:**
   - Sem ele, IRSA n√£o funciona
   - Deve ser criado antes do m√≥dulo ALB Controller
   - Provider TLS necess√°rio para thumbprint

2. **Helm provider precisa de cluster ativo:**
   - N√£o pode ser usado em `terraform plan` se cluster n√£o existe
   - Neste caso, cluster j√° existia (Marco 1)

3. **ALB provisioning leva 1-2 minutos:**
   - Target registration: ~10 segundos
   - ALB state "provisioning" ‚Üí "active": ~20 segundos
   - DNS propagation: pode levar at√© 60 segundos
   - Sempre validar target health antes de testar HTTP

4. **Controller √© event-driven:**
   - Monitora Ingress resources via Kubernetes API
   - Cria/atualiza/deleta ALBs automaticamente
   - Logs muito claros (JSON structured logging)

5. **Terraform state locking funciona perfeitamente:**
   - DynamoDB table do Marco 0 √© compartilhada
   - Cada Marco tem seu pr√≥prio state file
   - Sem conflitos de lock

### üöÄ Pr√≥ximos Passos (Marco 2 - Fases Seguintes)

Conforme documentado no [README.md](../../../README.md), as pr√≥ximas etapas do Marco 2 s√£o:

2. **Cert-Manager** - Certificados TLS automatizados
3. **Prometheus + Grafana** - Monitoramento de m√©tricas
4. **Fluent Bit + CloudWatch** - Logging centralizado
5. **Network Policies** - Isolamento de rede
6. **Cluster Autoscaler/Karpenter** - Auto scaling de nodes
7. **Aplica√ß√µes de teste** - Valida√ß√£o end-to-end

### üìå Estado Atual do Projeto

**Marcos conclu√≠dos:**
- ‚úÖ Marco 0: Backend Terraform + VPC reverse engineering
- ‚úÖ Marco 1: Cluster EKS com 7 nodes e 4 add-ons
- üü° Marco 2: AWS Load Balancer Controller (Fase 1 de 7)

**Pr√≥xima a√ß√£o:**
- Implementar Cert-Manager (Marco 2 - Fase 2)

---

Sess√£o conclu√≠da em: 2026-01-26 14:10 UTC
Tempo total da sess√£o: ~35 minutos
